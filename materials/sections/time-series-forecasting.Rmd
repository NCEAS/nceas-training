---
author: "David LeBauer"
date: "10/22/2021"
---


## Learning Objectives

In this session you will learn how to

- Simulate data and test if your model is working as expected
- Use linear regression to fit and predict time series
- Visualize data
- Decompose a time series into seasonal, trend, and error

As a reminder, these lessons are intended to provide a high level introduction to concepts and tools used in time series analysis and forecasting. While it is not possible to provide the background required to fully understand the theory and application of these methods in this short time, learners are pointed to references betlow. 

I would recommend starting with the  ["Forecasting Principles and Practice v2"](https://otexts.com/fpp2/) by Rob Hyndman who wrote the R `forecast` package. 

Hydnman and others are creating the successor to the forecast package called 'feasts' intended to align more closely with the 'tidymodels' and 'tidyverse' ecosystems. The next version of the Hyndman text has code using the feasts package https://otexts.com/fpp3/. At the time of writing, the forecast package is both more stable and more widely used.

## Linear regression, with time as a predictor

We start with regression using time as the dependent variable.

Both the simulation of data and the fitting of the model here is a general approach. It is demonstrated with a simple regression model of increasing $Y$ as a funciton of time. But simulating data can be very useful in understanding your data, your model, and the about statistics in general. 

First, lets define our data model.

$$
Y_{\textrm{linear}}=1+2t +\epsilon\\
\epsilon\sim N(0,1)
$$

Lets imagine that $Y$ is the mass of your favorite organism. Say, a fish, born on day 0 with a mass of $1$ gram (the intercept) and growth rate of $2\pm 1$ grams per day.   

First, simulate and plot the growth a fish:

```{r}
library(forecast)
library(dplyr)
library(lubridate)
library(broom)
set.seed(103)

days <- 0:9
mass <- vector(length = 10)
for(t in seq_along(days)){
  mass[t] <- 1 + 2 * days[t] + rnorm(1, 0, 1)
}


linear_data <- data.frame(day = days, mass = mass)
plot(mass~day, 
     data = linear_data, 
     ylab = 'mass (g)', 
     ylim = c(1, 20),
     xlim = c(0, 15))

```

Now, lets fit a linear regression. It will allow us to estimate the parameters - and to see whether we can recover the 'truth' of the growth model that generated the observations. Did the model find the right values?

```{r}
mod_linear <- lm(mass ~ 1 + days, data = linear_data)
tidy(mod_linear)
```

Did we recover the correct parameters from the simulated data?

Don't forget to check assumptions! `plot(lm)`
This is helpful when you use simulated data, because it helps you to see how these diagnostics 
look even with a known data generating process.


```{r}
mod_linear

## you can look at model statistics 
summary(mod_linear)



# plot(mod_linear)
```


### Further notes: a digression from time into the power of simulation for power analysis

The data generated above has a random error term $\epsilon\sim N(0,1)$. You can use simulation for power analysis by simulating a lot of datasets and fitting the model to test, e.g., what percent of the datasets have a particular P value, or if the mean parameter estimates (intercept, slope, error in this case) converge on the 'true' values.

The concept of a 'data generating process' is an important concept when modeling, and when synthesizing data from many different locations. A 'data generating process' includes both the system being studied and the tools used to observe (discussed further below in the section on state space models). It is common to hear data referred to as the truth. But data can only represent an incomplete view of the system itself.  In the end both data and models are representations of a system that help estimate some underlying truth.

```{r}

n_sims <- 1000
sim_data <- list()
fits <- list()
days <- 0:9
n <- length(days)


for(i in 1:n_sims){
  sim_data[[i]] <- vector(mode = 'double', length = n)
  for(t in seq_along(days)){
    mass[t] <- 1 + 2 * days[t] + rnorm(1, 0, 1)
  }
  sim_data[[i]] <- mass
  fits[[i]] <- lm(mass ~ 1 + days)
}
```



Now, we can create a data frame of the statistics that we are interested in. Here, we will look at fitted values of:

1. intercept
2. slope
3. model error
4. significance (P value) of test that intercept is different than 0
5. significance of test that slope is different than 0


```{r}
v <- vector(length = n_sims)
stats <- data.frame(intercept = v, slope = v, sigma = v, p_i = v, p_s = v)
for(i in 1:n_sims){
  stats[i, c('intercept', 'slope')] <- coef(fits[[i]])
  stats[i, 'sigma']                 <- sigma(fits[[i]])
  stats[i, c('p_i', 'p_s')]         <- summary(fits[[i]])$coefficients[1:2, 4]
}


```

Now, we can check out whether our model returns the simulated terms. Here we will look at the mean and sd, and create a nice table while we are at it! 

```{r}
data.frame(slope = c(mean(stats$slope), sd(stats$slope)), 
           intercept = c(mean(stats$intercept), sd(stats$intercept)), 
           row.names = c('mean', 'sd'))

```

Now that we are working with our 'population' of model fits, we can use all manner of statistics and visualizations to better understand the simulated experiments. We can use `pairs`, `hist`, `quantile` and others. 

For the classical 'power analysis' lets look at the percent of time the slope term is significant at 
P <= 0.05. How often is the slope significant? How often is the intercept significant?

This is rather straightforward with the 'stats' data.frame we created above:

```{r}

sum(intercept_ps > 0.05)/length(day_ps)
sum(day_ps > 0.05)/length(day_ps)
```



_note_ (use P values with care, if at all! Here they are used because they are a familiar and still commonly used statistic) But see the [Forum on P values and Model Selection in Vol 95, Issue 3 of Ecology](https://esajournals.onlinelibrary.wiley.com/toc/19399170/2014/95/3).

Now back to the main topic!

## Time series 'feature extraction'

We've already estimated some of the statistical properties of our time series. In machine learning, these parameter estimates can be used as a set of numbers with lower dimension than the raw data. Once you have estimated a slope and intercept, you have condensed your dataset from 10 points into two values that describe the data. This is a simple way of creating features for a machine learning model from even complex time series.

There are a lot of other mathematical functions that can be fit to your data, and properties of these functions that can be extracted, and used in subsequent analyses.

Bolker (2007) provides a 'bestiary of functions' in chapter 3 of his book ["Ecological Models and Data in R"](https://ms.mcmaster.ca/~bolker/emdbook/book.pdf). It provides some biological meanings and useful contexts for a variety of linear and non-linear, continuous and discontinuous functions. 

This is a common and very useful way of converting a time series into descriptive statistics.

For an example of Bayesian logistic regression using JAGS, see the work done by Jessica Guo and me: https://github.com/genophenoenvo/JAGS-logistic-growth. 

Dietze's Ecological forecasting course shows how to simulate a logistic growth curve using Monte Carlo methods. https://github.com/EcoForecast/EF_Activities/blob/master/Exercise_02_Logistic.Rmd 

There are a number of other methods for 'dimensionality reduction'. Principal components analysis is an important one. Another is taking a full resolution image and reducing the resolution - even from thousands to hundreds or even tens of pixels. In the machine learning world a wide range of methods are used in a process that they call 'encoding'.

### The 'predict' function: forecasting with a fitted regression model

The `predict` function in R has a variety of uses. Last week we learned how to use it to impute missing data values. This week, we will learn how to use it to predict the next time points in our time series. 

As an example, given that we have observed a fish on the first ten days (day 0 to day 9) of its life, and then fit the model on day 11, lets predict what the mass of our fish over the next five days. 

We will use the fitted slope and intercept to plot a single value on the regression line for each day. 
We will plot the data in black, the new points in red, and the regression line.

```{r}

newdays <- 11:15
newdat <- data.frame(days = newdays)
preds <- predict(mod_linear, 
                 newdata = newdat)
## approx. equivalent to:
# preds <- 1 + 2 * newdays

plot(mass ~ day, 
     data = linear_data, 
     ylab = 'mass (g)', 
     ylim = c(1, 30),
     xlim = c(0, 15)) +
  points(newdays, preds, col = 'red') +
  abline(coef(mod_linear)) + 
  abline(confint(mod_linear)[,1], lty = 2) +
  abline(confint(mod_linear)[,2], lty = 2)

```

Now, we have predicted the next five days of growth! 

## Time Series objects in R

R has a specific type of 'object' that it uses for time series. It is called a time series object - 'ts' (or 'mts' for a multivariate time series). These objects are analogous to a 'data.frame', but are special because they contain information describing the time between each observation.  See `?ts`

Lets create time series objects so that we can use some of the basic functions for time series analysis.

**How to make a time series object.**

Last week when we learned about the `imputeTS` package, we skipped over what we actually did to make a time series object. 

We set the start and end by year and index, and then either a time step 'deltat' or frequency. 

Values of frequency (per year):
- daily data: 365
- monthly: 12
- hourly: 365*24 
- etc

Lets make some sample monthly time series for three cases

1. for white noise: $Y=\epsilon$
2. with autocorrelation with a 1 month lag $Y_t=\frac{Y_{t-1}+Y_{t}}{2}$
3. with autocorrelation and a trend $Y_t=\frac{Y_{t-1}+Y_{t}}{2} + \frac{t}{48}+\epsilon$
4. with seasonal patterns $Y_t=\sin(\frac{2\pi t}{12})+\epsilon$
5. with seasonal patterns and a trend $Y_t=\sin(\frac{2\pi t}{12})+\frac{t}{48} + \epsilon$

Where $t$ is the time step (in units of months). The second equation takes the average of the last time step and the current one (a moving window of size 2). The third equation adds a trend - every month the value increases by 1/120. 

```{r}
set.seed(210)
months <- 1:240
noise <- rnorm(length(months))

lag   <- vector() 
for(t in 1:length(months)){
  if(t == 1){
    lag[t]  <- rnorm(1)
  } else {
    lag[t]  <- (lag[t-1] + noise[t]) / 2
  }
}

lag_trend <- lag + months / 48

seasonal <- 2*sin(2*pi*months/12) + noise

seasonal_trend <- seasonal + months / 48 

```

Now lets create the multivariate time series object:

```{r}
all <- ts(data = data.frame(noise, lag, lag_trend, seasonal, seasonal_trend), 
          frequency = 12)

plot(all)
```


Lets look at some basic statistics: which of these have a lag?

This plot will show the correlation between $Y_t$ and $Y_{t-\textrm{lag}}$

```{r}
lag.plot(all, set.lags = c(1, 3, 6, 9))

```

### Autocorrelation

Autocorrelation measures the correlation between an observation at time $t$ and 
previous observations at defined 'lags', e.g. the correlation between $Y_t$ and $Y_{t-1}$
would be an autocorrelation with a lag of 1.

```{r}
acf(all[,'noise'], xlab = 'Lag (years), noise')
acf(all[,'lag'], xlab = 'Lag (years), lag 1')
acf(all[,'lag'], xlab = 'Lag (years), lag 1 and trend')
acf(all[,'seasonal'], xlab = 'Lag (years), seasonal')

```

Learn more in chapter 2 of [Hyndman's text](https://otexts.com/fpp2/autocorrelation.html),

### Time Series Decomposition

We want to look at a time series in terms of its components.

To do so, we will review a number of handy functions for basic time series analysis that are provided in base R and the forecast package.

First, we will use the `decompose` function to extract and visualize some of the basic components of a time series. These components are:

- seasonal patterns
- trend
- residuals

```{r fig.height=5}
dec <- decompose(all[,'seasonal_trend'])

plot(dec)
```


This plot shows the four panels. First is the raw data, the the next three panels show the trend, seasonal pattern, and the residuals. These can be used in further statistical analysis. For example, you may be interested in learning about other variables that explain patterns in the the residuals after the trend and seasonal patterns are removed. 

Lets simulate the number of times the fish sees a twig float by, and see if that affects our time series:

```{r}
r <- rpois(n = length(dec$random), lambda = 4)
tidy(lm(dec$random ~ r, data = dec_df))
```
No, there is no relation!(as expect since this random number generating process is unrelated to the time series)

Or, you may want to test whether the trend is positive by regressing it against the months:


```{r}

tidy(lm(dec$trend ~ months))

```
### Seasonal Trend with Loess smoothing (STL) decomposition

The seasonal component of the decomposed time series is very regular. The `acf()` function creates a seasonal component using the means, so if you have a monthly data set, the seasonal component for January will be the mean of all historical Januarys. 

At a more sophisticated approach is to weight the mean with more recent data - so, e.g. this month will be the historical mean adjusted to the recent trends. One algorithm provided in base R is `stl` which performs seasonal decomposition with local smoothing using Loess. 

Seasonal trend w/ local smoothing Loess (STL) to smooth over a few years.

Some guidelines:

- the trend window should be greater than the seasonal window
- there are rules of thumbs for estimating the parameters
- The `forecast` and `auto.arima` functions can automatically fit these parameters 

```{r}
seasonal_stl <- stl(all[,'seasonal_trend'], s.window = 6)
plot(seasonal_stl)
## note how you can access each component of the decomposed time series 
## plot(seasonal_stl$time.series[,c('trend', 'seasonal', 'remainder')])
```

### Now we can analyze the trend

Is there a trend using local smoothing?

```{r}
fit <- lm(seasonal_stl$time.series[,'trend'] ~ months)
coef(fit)

```
What does that coefficient 'month' mean?

<details>
Month is the rate of change per month. Check the equation above that we used to generate the time series. What is the slope of the trend that we added?
</details>

### Some Time Series Model Acronyms

**ARIMA Models**

You will often see time series models specified as **ARIMA(p,d,q)**

**AR: AutoRegressive time series**
**I: Integrated**
**MA: Moving Average smoothing**

- _p_ number of lags 
- _d_ degree of differencing
- _q_ size of moving average window

The `forecast::auto.arima` function automatically fits these parameters. See https://otexts.com/fpp2/arima-r.html for an explanation of how this function works.

**STL** Seasonal Seasonal Decomposition of Time Series by Loess. Extracts trend, seasonal, and locally smoothed moving average (described above). 

**ETS** Exponential smoothing state space model. These forecast based on previous observations, but the importance of the prior observations are weighted using exponential smoothing such that more recent observations have more weight. See https://otexts.com/fpp2/expsmooth.html


### Tools for forecasting

As we discussed last week, there are some simple or 'naiive' ways to forecast - using the historical means from a seasonal trend, or using the overall mean. These approaches are useful, sometimes because they perform well and other times because they provide a reasonable null hypothesis. Some use the same methods that we used to interpolate.

The `forecast` package has a lot of handy functions for the analysis of time series data, and the fitting of complex models including components described above. Here are some that may be useful:

There are also functions `ets`, `stlm`, `ma`, `spline`, `meanf`, and `snaive` for ETS, STL, Moving Average, Spline, and Naive forecasts. 

The `tslm` function uses `lm` and adds the ability to automatically specify trend and season componets as predictors (even if they aren't in the data frame). The funciton fits a linear regression to trend, season, and other factors. This has special terms 'trend' and 'season' that don't actually have to be in the data frame - they are automatically fit and added. The result will include one estimate for the trend, and another for each time step (in this case, an intercept for the first month and a parameter for each subsequent month):  

```{r}
ts_fit <- tslm(all[,'seasonal_trend'] ~ trend + season)
plot(forecast(ts_fit, h = 20))
tidy(ts_fit)

```

Another handy funciton is `auto.arima`. This function will evaluate a large number of candidate models (e.g. with different parameters for the STL and ETS components) and select the best based on AIC or other criteria. Note that this takes a long time because it has to evaluate so many candidate models. 


```{r}
auto_fit <- auto.arima(all[,'seasonal_trend'])
summary(auto_fit)
```


### State Space Models

State space models are models of how the state of a system changes over time.  

State space models leverage a useful concept of a "latent variable". A latent variable is the process of interest, but which can not be directly measured. While it can not be directly measured, it can be measured with an 'observation model'. This decoupling of the measurement from the process of interest is a conceptual step that is very helpful. 

Indeed, whatever is used to actually measure - often boils down to changes in an electric current that capture a property of interest. If we are measuring temperature with a mercury thermometer, there is an 'observation' model (scale on the thermometer) that converts the observation of the height of mercury into the latent variable of air temperature.

A common application of state space modeling is to the study of dynamical systems. Often such a model is a set of equations that describe how a system evolves in time. 

As a simple example, consider a model of a pendulum (see [Wikipedia](https://en.wikipedia.org/wiki/Pendulum_(mechanics)). This model has terms for the force of gravity and the length of the pendulum. The observatino might be the location of the pendulum at multiple points in time. It is then possible to estimate the force of gravity, the torque, velocity, and potential and kinetic energy at any point in time by fitting the observations of position to the dynamical model of pendulum dynamics.

Such a model can take parameters for equations that control the the dynamics of the system (e.g 'growth rate'). you know the state of a system, and how the system changes in time, you can use a state-space framework. Once the model represents the current state, it can represent the state of a system. Conceptually, they are a way of modeling time series when you have a model of the 'process' or mechanism that moves the system state from $t$ to $t+1$.

The 'state' of the system may be partially unobserved. But we can infer these states based on observations. 



## Ecological Forecasting

Like time series, forecasting is a very large area of research. 

Ecological forecasting is an emerging discipline, and it covers both basic research and the application of ecological understanding to applications. Forecasts can help provide insights into the future state of a system as well as provide guidance on management scenarios. 

Some examples of ecological forecasting problems:

- Where is the hurricane going to end up?
- How much carbon can the land store?
- How will flooding affect delta planton and fish populations?
- Predict the potential yield of different crops under future climates
- Predict forest green up and senescence

![](images/davis-agave.png)


![](images/efi_phenology.png)

Key references include Clark et al 2001 and Dietze et al 2018 below. The Ecological Forecasting Initiative (EFI, [ecoforecast.org](https://ecoforecast.org/)) is a Research Coordination Network that you can join to learn and contribute to the development of this field.


### Forecasting Challenges

There are a variety of forecasting challenges. These can be found on sites like Kaggle. Lets look at a few

- [EFI NEON forecasting Challenge](https://ecoforecast.org/efi-rcn-forecast-challenges/)
  - phenology, net ecosystem exchange, beetle abundance, water temperature and dissolved oxygen
- Kaggle: [Predict end of season Sorghum biomass from photograps](https://www.kaggle.com/c/sorghum-biomass-prediction/)
- HiveMind / Agrimetrics UK Wheat yield forecast market [Yield 21](https://agrimetrics.co.uk/2021/10/20/uk-2021-wheat-yield-prediction/)


A note on challenges. Framing a problem as a challenge is a great way to engage the machine learning world. And these challenges provide a way to engage other communities, and lower the barrier to entry and more level playing field than many scientific pursuits. For challenges like those on Kaggle and the ones run by EFI, the best model wins. 



## Your Turn - Some Actual Data!

### Example 1: A twenty year history of weather in Maricopa, AZ

Here, we will work with daily weather statistics from 'DayMet'. Daymet isn't actually observed data - it is 'imputed' data. So, it is available for everywhere in the continental US from 1980 on a 1km grid. You can learn more about it here: https://daymet.ornl.gov/ and Thornton et al 2021. We will also use this dataset in the model analysis lesson to compare this imputed 'model' data with ground truth observations.

While we have a sample of the dataset in the lesson repository for a particular site, it is useful to know how to access the data for your site.

Just change the lat, lon, start and end times to look at your favorite site!


```{r eval=FALSE}
library(daymetr)
mac_daymet_list <- download_daymet(site = "Maricopa Agricultural Center",
                                   lat = 33.068941,
                                   lon =  -111.972244,
                                   start = 2000,
                                   end = 2020, internal = TRUE)


# rename variables, create a date column
mac_daymet <- mac_daymet_list$data %>% 
    transmute(date = ymd(paste0(year, '01-01'))+ days(yday) -1, 
              precip = prcp..mm.day., 
              tmax = tmax..deg.c.,
              tmin = tmin..deg.c.,
              tmean = (tmax + tmin) / 2,
              trange = tmax - tmin,
              srad = srad..W.m.2.,
              vpd = vp..Pa.) 
readr::write_csv(mac_daymet, file = '../data/mac_daymet.csv')

```

Lets read in and look at the data

```{r}
mac_daymet <- readr::read_csv('../data/mac_daymet.csv') %>% 
    select(date, precip, tmean, srad, vpd)

head(mac_daymet)
```

Now lets create a univariate time series object of mean temperature and a multivariate time series 
object of all of the variables so that we can apply some of the functions we learned about above to some simulated data.

```{r}
tmean.ts <- ts(mac_daymet$tmean, 
               start = c(2000, 1), 
               end = c(2020, 365), 
               deltat = 1/365)

mac_ts <- ts(mac_daymet, 
               start = c(2000, 1), 
               end = c(2020, 365), 
               deltat = 1/365)

```

Lets create some plots to explore these data:

```{r}
plot(tmean.ts, ylab = "Daily mean T", xlab = "Year")
```
Lets look at some of the lags in the time series of temperature. The lags we look at will be 
1 day, 1 week, 1 month, 1 quarter, 1/2 year and 1 year.

```{r}
lag.plot(tmean.ts, set.lags = c(1, 7, 30, 90, 180, 360))
```

Which of these lags are positively correlated? 

<detalis>The day, week, and month are all positively correlated.</details> 

Which are negatively correlated 
<details>The 1/2 year is negatively correlated - it will be warm on July 1 and cold on Dec 1</details>

What causes the donut?

<details>

Imagine a circle from trigonometry class. If the value of x at the top of the circle is at 0 degrees, the equivalent of 90 days or 1/4 year ago would be the value of x at -90 degrees. Move forward to +90 degrees and the value of x is 0; the value of x 1/4 of the way back is 1. Take a second to plot these out. 

```{r, fig.height=4, fig.width=4}
angle <- 1:13 * 2 * pi/12

x <- sin(angle)
x_lag <- c(x[10:12], x[1:10])
plot(x, type = 'l') + lines(x_lag, lty = 2)
plot(x, x_lag, type = 'l')
```
</details>

### Autocorrelation 

The lag plot shows the correlation between each point and the points at t-lag.

Since there are so many points above, lets calculate the monthly temperature averages; this will make it easier to work with.


```{r}
tmean_mo <- mac_daymet %>% 
    mutate(year = year(date), month = month(date)) %>% 
    group_by(year, month) %>% 
    summarise(tmean = mean(tmean), .groups = 'keep') %>% 
    ungroup() %>% 
    select(tmean)

tmean.mo.ts <- ts(tmean_mo, start = c(2000, 1), end = c(2020, 12), frequency = 12)

lag.plot(tmean.mo.ts, lags = 12)


```
Now we can see how lag plots work over the course of twelve months. Lets look at the autocorrelation plot:

```{r}
plot(acf(tmean.mo.ts), xlab = 'Time (years)')
```



Your turn - decompose, plot, look at a lag plot, acf, fit using auto.arima. 

which of the other variables are similar to temperature? Which are most different?

```{r}
vpd_mo <- mac_daymet %>% 
    mutate(year = year(date), month = month(date)) %>% 
    group_by(year, month) %>% 
    summarise(vpd = mean(vpd), .groups = 'keep') %>% 
    ungroup() %>% 
    select(vpd)

lag.plot(vpd_mo, lags = 12)
acf(vpd_mo)

ccf(tmean_mo, vpd_mo)
```
**Your Turn:**

Plot a few of the other variables. How do the seasonal patterns and trends compare?

```{r}
precip.ts <- ts(mac_daymet$precip, 
               start = c(2000, 1), 
               end = c(2020, 365), 
               deltat = 1/365)
lag.plot(precip.ts)
vpd.ts <- ts(mac_daymet$vpd, 
               start = c(2000, 1), 
               end = c(2020, 365), 
               deltat = 1/365)

lag.plot(vpd.ts, )

all.ts <- ts(mac_daymet, 
               start = c(2000, 1), 
               end = c(2020, 365), 
               deltat = 1/365)
lag.plot(all.ts)

ccf(vpd.ts, precip.ts)
ccf(precip.ts, tmean.ts)

```


Can you determine if there is an impact of solar radiation and vpd on temperature after accounting for trend and seasonal components? 


<details>
There are a few ways.

```{r}
d <- decompose(tmean.ts)
mvlm <- tslm(d$random ~ srad + vpd + srad:vpd, data = mac_ts)


summary(mvlm)

```
</details>

OK, now can you do the same using `stl` decomposition?

<details>
```{r}

s <- stl(tmean.ts, s.window = 'periodic')
r <- seasadj(d)
stlmvlm <- tslm(r ~ srad + vpd + srad:vpd, data = mac_ts)
summary(stlmvlm)
```
</details>



## References

**Text Books**
Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on 2021-10-31

Dietze, Michael. Ecological forecasting. Princeton University Press, 2017.

**Courses**

Ethan White and Morgan Earnst Ecological Dynamics and Forecasting  https://github.com/weecology/forecasting-course


Dietze, Michael. Ecological forecasting. Course materials https://github.com/EcoForecast/EF_Activities


**DayMet** 

Thornton, M.M., R. Shrestha, Y. Wei, P.E. Thornton, S. Kao, and B.E. Wilson. 2020. Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1840

Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Version 4 https://doi.org/10.3334/ORNLDAAC/1840

**Other Publications**

Dietze, Michael C., et al. "Iterative near-term ecological forecasting: Needs, opportunities, and challenges." Proceedings of the National Academy of Sciences 115.7 (2018): 1424-1432. https:doi.org/10.1073/pnas.1710231115
