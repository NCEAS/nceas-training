---
author: "Amber Budden"
---

## Introduction to Reproducible Research

### What is research reproducibility and how does it relate to open science? 

Reproducibility is a hallmark of scientific research, which is based on empirical observations coupled with explanatory models. Whether integrating data from across multiple studies and sources, or working with your own data, the data life cycle typically involves some degree of data collection/integration, quality assurance practices, analysis and synthesis.  Operating in a reproducible fashion means that each of these steps can be easily re-executed to achieve the same result, ideally as part of a single workflow.  

Reproducibility means different things to different researchers. For our purposes, practical reproducibility looks like:

- Preserving the data
- Preserving the software workflow
- Documenting what you did
- Describing how to interpret it all

Reproducibility does not, by definition, require openness. Reproducibility can be achieved within a single research activity or across a research program with a closed group of collaborators. However, when working in an OPEN and REPRODUCIBLE manner, we are better able to transform knowledge into benefits for society. In this section we will expand on the benefits of reproducible research and open science before highlighting some best practices. 

### Why is reproducible research important? {-}

Working in a reproducible manner builds efficiencies into your own research practices. The ability to automate processes and rerun analyses as you collect more data, or share your full workflow (including data, code and products) with colleagues, will accelerate the pace of your research and collaborations. However, beyond these direct benefits, reproducible research builds trust in science with the public, policy makers and others.  

![](images/Smith-et-al.png)

What data were used in this study? What methods applied? What were the parameter settings? What documentation or code are available to us to evaluate the results? Can we trust these data and methods? 

Are the results reproducible?

![](images/OSC.png)

Ionnidis (2005) contends that "Most research findings are false for most research designs and for most fields", and a study of replicability in psychology experiments found that "Most replication effects were smaller than the original results" (Open Science Collaboration, 2015).

![](images/NCA.png)

In the case of 'climategate', it took three years, and over 300 personnel, to gather the necessary provenance information in order to document how results, figures and other outputs were derived from input sources. Time and effort that could have been significantly reduced with appropriate documentation and reproducible practices. Moving forward, through reproducible research training, practices, and infrastructure, the need to manually chase this information will be reduced enabling replication studies and great trust in science.

### Open Science

To enable full reproducibility by the broader community; researchers, practitioners, policy makers etc, all products of the research activity need to be accessible - open data, open code, and open publications. Further, full research transparency also requires open peer review. There are, of course, data sensitivities and ethical considerations regarding open everything and these will be discussed later.

At its core, the aims of Open Science are to:

1. Increase transparency of the research process
1. Enable reproducibility of results and conclusions
1. Accelerate Discovery
1. Enhance and facilitate collaboration
1. Increase diversity, equity and inclusion
1. Transform knowledge into benefits for society

#### Computational reproducibility {-}

Computational reproducibility is the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions.

To be able to evaluate the data, analyses, and models on which conclusions are drawn, computational reproducibility requires open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., @hampton_tao_2015, @munafo_manifesto_2017).

Scientific workflows encapsulate all of the steps from data acquisition, cleaning,
transformation, integration, analysis, and visualization.  

![](images/workflow.png)

Workflows can range in detail from simple flowcharts 
to fully executable scripts. R scripts and python scripts are a textual form 
of a workflow, and when researchers publish specific versions of the scripts and 
data used in an analysis, it becomes far easier to repeat their computations and 
understand the provenance of their conclusions.

Computational reproducibility provides:

- transparency by capturing and communicating scientific workflows
- research to stand on the shoulders of giants (build on work that came before)
- credit for secondary usage and supports easy attribution
- increased trust in science

Preserving computational workflows enables understanding, evaluation, and reuse for the benefit of *future you* and your collaborators and colleagues **across disciplines**.

### Community Principles: CARE and FAIR 

The CARE and FAIR Principles were introduced previously in the context of introducing the Arctic Data Center and our data submission and documentation process. In this section we will dive a little deeper.

To recap, the Arctic Data Center is an openly-accessible data repository and the data published through the repository is open for anyone to reuse, subject to conditions of the license (at the Arctic Data Center, data is released under one of two licenses: [CC-0 Public Domain](https://creativecommons.org/publicdomain/zero/1.0/) and [CC-By Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)).  In facilitating use of data resources, the data stewardship community have converged on principles surrounding best practices for open data management  One set of these principles is the [FAIR principles](https://force11.org/info/guiding-principles-for-findable-accessible-interoperable-and-re-usable-data-publishing-version-b1-0/). FAIR stands for Findable, Accessible, Interoperable, and Reproducible.


![](images/FAIRsFAIR.png)

The “[Fostering FAIR Data Practices in Europe](https://zenodo.org/record/5837500#.Ygb7VFjMJ0t)” project found that it is more monetarily and timely expensive when FAIR principles are not used, and it was estimated that 10.2 billion dollars per years are spent through “storage and license costs to more qualitative costs related to the time spent by researchers on creation, collection and management of data, and the risks of research duplication.” FAIR principles and open science are overlapping concepts, but are distinctive concepts. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. 


![](images/FAIR_CARE.png)

Another set of community developed principles surrounding open data are the [CARE Principles](https://static1.squarespace.com/static/5d3799de845604000199cd24/t/5da9f4479ecab221ce848fb2/1571419335217/CARE+Principles_One+Pagers+FINAL_Oct_17_2019.pdf).  The CARE principles for Indigenous Data Governance complement the more data-centric approach of the FAIR principles, introducing social responsibility to open data management practices. The CARE Principles stand for:

- Collective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data 
- Authority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.
-  Responsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.
- Ethics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.

The CARE principles align with the FAIR principles by outlining guidelines for publishing data that is findable, accessible, interoperable, and reproducible while at the same time, accounts for Indigenous’ Peoples rights and interests. Initially designed to support Indigenous data sovereignty, CARE principles are now being adopted across domains, and many researchers argue they are relevant for both Indigenous Knowledge and data, as well as data from all disciplines (Carroll et al., 2021). These principles introduce a “game changing perspective” that encourages transparency in data ethics, and encourages data reuse that is purposeful and intentional that aligns with human well-being  aligns with human well-being (Carroll et al., 2021). 



### Data and Metadata Best Practices

We'll spend an entire lesson later on that's dedicated to organizing your data in a tidy and effective manner, but first, let's focus on the benefits on having "clean" data and complete metadata.

- Decreases errors from redundant updates
- Enforces data integrity
- Helps you and future researchers to handle large, complex datasets
- Enables powerful search filtering

Much has been written on effective data management to enable reuse. The following two papers offer words of wisdom:

- [**Some simple guidelines for effective data management.**](https://doi.org/10.1890/0012-9623-90.2.205) Borer et al. 2009. Bulletin of the Ecological Society of America. 
- [**Nine simple ways to make it easier to (re)use your data.**](https://doi.org/10.4033/iee.2013.6b.6.f) White et al. 2013. Ideas in Ecology and Evolution 6. 


#### Metadata Guidelines

Metadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection.
Imagine that you're writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what's inside your data files? 

The goal is to have enough information for the researcher to **understand** the data, **interpret** the data, and then **re-use** the data in another study.

Another way to think about it is to answer the following questions with the documentation:

- What was measured?
- Who measured it?
- When was it measured?
- Where was it measured?
- How was it measured? 
- How is the data structured?
- Why was the data collected?
- Who should get credit for this data (researcher AND funding agency)?
- How can this data be reused (licensing)?

*Bibliographic Details*

The details that will help your data be cited correctly are:

- a **global identifier** like a digital object identifier (DOI);
- a **descriptive title** that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data
- a **descriptive abstract** that serves as a brief overview off the specific contents and purpose of the data package
- **funding information** like the award number and the sponsor;
- the **people and organizations** like the creator of the dataset (ie who should be cited), the person to contact about the dataset (if different than the creator), and the contributors to the dataset

*Discovery Details*

The details that will help your data be discovered correctly are:

- the **geospatial coverage** of the data, including the field and laboratory sampling locations, place names and precise coordinates;
- the **temporal coverage** of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to; 
- the **taxonomic coverage** of the data, including what species were measured and what taxonomy standards and procedures were followed; as well as
- **any other contextual information** as needed.

*Interpretation Details*

The details that will help your data be interpreted correctly are:

- the **collection methods** for both field and laboratory data;
- the full **experimental and project design** as well as how the data in the dataset fits into the overall project;
- the **processing methods** for both field and laboratory samples IN FULL;
- all **sample quality control** procedures;
- the **provenance** information to support your analysis and modelling methods;
- information about the **hardware and software** used to process your data, including the make, model, and version; and
- the **computing quality control** procedures like any testing or code review.

*Data Structure and Contents*

Well constructed metadata also includes information about the data structure and contents. Everything needs a description: the data model, the data objects (like tables, images, matricies, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.

Variable information includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (such as 0 = not collected), and any missing values (such as 999 = NA).

Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The [semantics](https://arcticdata.io/semantic-annotations/) of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).

For example, if you were to search for the character string `carbon dioxide flux` in the general search box at the Arctic Data Center, not all relevant results will be shown due to varying vocabulary conventions (ie, `CO2 flux` instead of `carbon dioxide flux`) across disciplines — only datasets containing the exact words `carbon dioxide flux` are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it `CO2 flux` WOULD be included in that search.


![](images/semantic-search.png)


Demonstrates a typical search for “carbon dioxide flux”, yielding 20 datasets. (right) Illustrates an annotated search for “carbon dioxide flux”, yielding 29 datasets. Note that if you were to interact with the site and explore the results of the figure on the right, the dataset in red of Figure 3 will not appear in the typical search for “carbon dioxide flux.”

*Rights and Attribution*

Correctly assigning a way for your datasets to be cited and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:

- the **citation format** to be used when giving credit for the data;
- the **attribution expectations** for the dataset;
- the **reuse rights**, which describe who may use the data and for what purpose;
- the **redistribution rights**, which describe who may copy and redistribute the metadata and the data; and
- the **legal terms and conditions** like how the data are licensed for reuse.

So, how do you organize all this information? There are a number of **metadata standards** (think, templates) that you could use, including the [Ecological Metadata Language (EML)](https://eml.ecoinformatics.org/), [Geospatial Metadata Standards](https://www.fgdc.gov/metadata/iso-standards) like ISO 19115 and ISO 19139, the [Biological Data Profile (BDP)](https://www.fgdc.gov/standards/projects/FGDC-standards-projects/metadata/biometadata/biodatap.pdf), [Dublin Core](https://dublincore.org/), [Darwin Core](https://dwc.tdwg.org/), [PREMIS](https://www.loc.gov/standards/premis/), the [Metadata Encoding and Transmission Standard (METS)](http://www.loc.gov/standards/mets/), and the list goes on and on. The Arctic Data Center runs on EML.

### Data Identifiers

Many journals require a DOI - a digital object identifier - be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.

At the Arctic Data Center, we assign a DOI to each published dataset. But, sometimes datasets need to be updated. Each version of a dataset published with the Arctic Data Center has a unique identifier associated with it. Researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available. When there is a newer version available, that will be clearly marked on the original dataset page with a yellow banner indicating as such.


![](images/newer-version.png)


Having the data identified in this manner allows us to accurately track the dataset usage metrics. The Arctic Data Center tracks the number of citations, the number of downloads, and the number of views of each dataset in the catalog. 


![](images/dataset-metrics.png)


We filter out most views by internet bots and repeat views within a small time window in order to make these metrics COUNTER compliant. COUNTER is a standard that libraries and repositories use to provide users with consistent, credible, and comparable usage data. 


![](images/arctic-data-center/ADC-datacatalog2.png)


### Data Citation

Researchers should get in the habit of citing the data that they use - even if it's their own data! - in each publication that uses that data. The Arctic Data Center has taken multiple steps towards providing data citation information for all datasets we hold in our catalog, including a feature enabling dataset owners to directly register citations to their datasets. 

We recently implemented this “Register Citation” feature to allow researchers to register known citations to their datasets. Researchers may register a citation for any occasions where they know a certain publication uses or refers to a certain dataset, and the citation will be viewable on the dataset profile within 24 hours. 

To register a citation, navigate to the dataset using the DOI and click on the citations tab. Once there, this dialog box will pop up and you'll be able to register the citation with us. Click that button and you'll see a very simple form asking for the DOI of the paper and if the paper **CITES** the dataset (that is, the dataset is explicitly identified or linked to somewhere in the text or references) or **USES** the dataset (that is, uses the dataset but doesn't formally cite it).


![](images/register-citation.png)

We encourage you to make this part of your workflow, and for you to let your colleagues know about it too!

### Provanance & Preserving Computational Workflows

![](images/comp-repro.png)

While the [Arctic Data Center](https://arcticdata.io), [Knowledge Network for Biocomplexity](https://knb.ecoinformatiocs.org), and similar repositories do
focus on preserving data, we really set our sights much more broadly on preserving
entire computational workflows that are instrumental to advances in science.  A 
computational workflow represents the sequence of computational tasks that are
performed from raw data acquisition through data quality control, integration, 
analysis, modeling, and visualization.

![](images/comp-workflow-1.png)

In addition, these workflows are often not executed all at once, but rather are
divided into multiple workflows, earch with its own purpose.  For example, a data
acquistion and cleaning workflow often creates a derived and integrated data product
that is then picked up and used by multiple downstream analytical workflows that
produce specific scientific findings.  These workflows can each be archived as 
distinct data packages, with the output of the first workflow becoming the input
of the second and subsequent workflows.

![](images/comp-workflow-2.png)

In an effort to make data more reproducible, datasets also support provenance tracking. With provenance tracking, users of the Arctic Data Center can see exactly what datasets led to what product, using the particular script or workflow that the researcher used.

![](images/provenance.png)

This is a useful tool to make data more compliant with the [FAIR](https://www.go-fair.org/fair-principles/) principles. In addition to making data more reproducible, it is also useful for building on the work of others; you can produce similar visualizations for another location, for example, using the same code.

RMarkdown itself can be used as a provenance tool, as well - by starting with the raw data and cleaning it programmatically, rather than manually, you preserve the steps that you went through and your workflow is reproducible.

![](images/fjord.png)
