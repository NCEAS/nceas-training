---
bibliography: book.bib
---

## Learning Objectives {.unnumbered}

-   Describe principles of tidy text
-   Employ strategies to wrangle unstructured text data into a tidy format using the `tidytext` package
-   Become familiar with text analysis (or text mining) methods and when to use them

::: callout-note
### Acknowledgements

This lesson has been adapted from the following resources:

-   [Welcome to Text Mining with R](https://www.tidytextmining.com/) by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/). Julia and David are also the developers of the [`tidytext`](https://juliasilge.github.io/tidytext/) package.
-   [Section 7.3: (R-) Workflow for Text Analysis from Computational Social Science: Theory & Application, Version: 17 June, 2021](https://bookdown.org/paul/2021_computational_social_science/r-workflow-for-text-analysis.html) by Paul C. Bauer
:::

## What is text data?

Text data is information stored as character or string data types. It comes in various different forms including books, research articles, social media posts, interview transcripts, newspapers, government reports, and much more.

::: {.callout-warning appearance="minimal"}
Raw text data is often unstructured and quite "messy". This could include wrong grammar, missing words, spelling issues, ambiguous language, humor, emojis, symbols, etc. Investing time into carefully cleaning and preparing text data is crucial for your ultimate analysis.
:::

### How do we talk about text data?

Here is a list of text data or text analysis terms we'll be referring to throughout this lesson. Note this is not a comprehensive list of text analysis terms that are used beyond this lesson.

| Term                              | Definition                                                                                                                                                                                       |
|-----------------|-------------------------------------------------------|
| Corpus (corpora, plural)          | Collection or database of text or multiple texts. These types of objects typically contain raw strings annotated with additional metadata and details.                                           |
| Document-term matrix              | Represents the relationship between terms and documents, where each row stands for a term and each column for a document, and an entry is the number of occurrences of the term in the document. |
| Natural Language Processing (NLP) | NLP is an interdisciplinary field used in computer science, data science, linguistics, and others to analyze, categorize, and work with computerized text.                                       |
| String                            | Specific type of data whose values are enclosed within a set of quotes. Typically values or elements are characters (e.g. "Hello World!").                                                       |
| Text analysis                     | The process of deriving high-quality information or patterns from text through evaluation and interpretation of the output. Also referred to as "text mining" or "text analytics".               |
| Token                             | A meaningful unit of text, such as a word, to use for analysis.                                                                                                                                  |
| Tokenization                      | The process of splitting text into tokens.                                                                                                                                                       |

### How is text data used in the environmental field?

As our knowledge about the environmental world grows, researchers will need new computational approaches for working with text data because reading and identifying all the relevant literature for literature syntheses is becoming an increasingly difficult task.

Beyond literature syntheses, quantitative text analysis tools are extremely valuable for efficiently extracting information from texts and other text mining or text analysis tasks.

Environmental researchers have used text data to:

-   Understand how text mining is used in the field [@farrell2022]
-   Gain insights into public perception of a particular subject [@froehlich2017]
-   Highlight the power of NLP models to advance research [@vanhoutan2020]
    

## What is tidy text data?

Let's recall what are the three tidy data principles:

1.  Every column is a variable.
2.  Every row is an observation.
3.  Every cell is a single value.

Keeping that in mind, Silge and Robinson define the tidy text format as **being a table with one-token-per-row**.

This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix.

For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.

By using tidy data principles, we can apply many "tidy" R packages including `dpylr`, `tidyr`, `ggplot2`, and more.

### What is the `tidytext` R package?

::: columns
::: {.column width="30%"}
![](images/tidytext-logo.png){fig-align="center"}
:::

::: {.column width="70%"}
`tidytext` is a package that applies the tidy principles to analyzing text. <br>

The package contains many useful functions to wrangle text data into tidy formats. It has been built considering other text mining R packages so that it's easy to switch between text mining tools (e.g. `tm`, `quanteda`, `stringr`, `wordcloud2`).
:::
:::

## Exercise: Tidy Text Workflow

![Source: Silge & Robinson](images/tidytext-tidy-workflow.png)

We are going to use the `gutenbergr` package to access public domain texts from [Project Gutenberg](https://www.gutenberg.org/) (a library of free eBooks). We'll then use the `tidytext`, `dyplr` and `ggplot2` packages to practice the tidy text workflow.

Break out into groups and then follow the exercise setup and instructions.

::: callout-tip
### Setup and Instructions

1.  Create a new `qmd` file and title it "Intro to Text Data", name yourself as the author, and then save the file as `intro-text-data.qmd`.

2.  Create a new code chunk and attach the following libraries:

```{r}
#| message: false
library(gutenbergr) # access public domain texts from Project Gutenberg
library(tidytext) # text mining using tidy tools
library(dplyr) # wrangle data
library(ggplot2) # plot data
```

3.  Depending on which group you're in, use one of the following public domain texts:

```{r}
#| eval: false
# Group A
gutenberg_works(title == "Dracula") # dracula text

# Group B
gutenberg_works(title == "Frankenstein; Or, The Modern Prometheus") # frankenstein text

# Group C
gutenberg_works(title == "The Strange Case of Dr. Jekyll and Mr. Hyde") # jekyll hyde text
```

4.  Get the id number from the `gutenberg_works()` function so that you can download the text as a corpus using the function `gutenberg_download()`. Save the corpus to an object called `{book-title}_corp`. View the object - is the data in a tidy format?

5.  Tokenize the corpus data using `unnest_tokens()`. Take a look at the data - do we need every single token for our analysis?

6.  Remove "stop words" or words that can be safely removed or ignored without sacrificing the meaning of the sentence (e.g. "to", "in", "and") using `anti_join()`. Take a look at the data - are you satisfied with your data? We won't conduct any additional cleaning steps here, but consider how you would further clean the data.

7.  Calculate the top 10 most frequent words using the functions `count()` and `slice_max()`.

8.  Plot the top 10 most frequent words using `ggplot()`. We reccommend creating either a bar plot using `geom_col()` or a lollipop plot using both `geom_point()` and `geom_segment()`.

9.  **Bonus**: Consider elements in `theme()` and improve your plot.
:::

### Example using *The Phantom of the Opera*

The code chunks below follows the instructions from above using the text of *The Phantom of the Opera*.

```{r}
# get id number
gutenberg_works(title == "The Phantom of the Opera")
```

```{r}
#| code-fold: true
#| code-summary: Steps 4-7 Code
#| message: false
#| eval: false
# access text data using id number from `gutenberg_works()`
phantom_corp <- gutenberg_download(175)

# tidy text data - unnest and remove stop words
tidy_phantom <- phantom_corp %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by = "word")

# calculate top 10 most frequent words
count_phantom <- tidy_phantom %>%
    count(word) %>% 
    slice_max(n = 10, order_by = n)
```

```{r}
#| code-fold: true
#| code-summary: Step 8 Plot Code
#| eval: false
# visualize text data #
# bar plot
ggplot(data = count_phantom, aes(n, reorder(word, n))) +
  geom_col() +
    labs(x = "Count",
         y = "Token")
```


<details>
<summary>Step 8 Bar Plot</summary>

![](images/tidytext-barplot.png)

</details>

```{r}
#| code-fold: true
#| code-summary: Step 9 Plot Code
#| eval: false
# visualize text data #
# initial lollipop plot
ggplot(data = count_phantom, aes(x=word, y=n)) +
    geom_point() +
    geom_segment(aes(x=word, xend=word, y=0, yend=n)) +
    coord_flip() +
    labs(x = "Token",
         y = "Count")

# ascending order pretty lollipop plot
ggplot(data = count_phantom, aes(x=reorder(word, n), y=n)) +
    geom_point(color="cyan4") +
    geom_segment(aes(x=word, xend=word, y=0, yend=n), color="cyan4") +
    coord_flip() +
    labs(title = "Top Ten Words in The Phantom of the Opera",
         x = NULL,
         y = "Count") +
    theme_minimal() +
    theme(
        panel.grid.major.y = element_blank()
    )
```

<details>
<summary> Step 9 Lollipop Plot</summary>

![](images/tidytext-lollipopplot.png)
</details>

## Tidy Text to Non-tidy Text Workflows

![A flowchart of a typical text analysis that combines `tidytext` with other tools and data formats, particularly the `tm` or `quanteda` packages. Source: Silge & Robinson](images/tidytext-nontidy-workflow.png)

In the [Tidy Text Workflow Exercise](#exercise-tidy-text-workflow), we converted our corpus into a data table that has "one-token-per-row". However, the tidy text format of one-token-per-row is not a common format for other R packages that work with text data or perform text analysis. Packages like `tm`, `quanteda`, `topicmodels`. 

Many text analysis methods, in particular NLP techniques (e.g. topic models) require text data to be stored in a mathematical format. A common approach is to create a matrix, such as a: sparse matrix, a document term matrix (DTM), or a document-feature matrix (DFM). In a matrix format, algorithms are able to more easily compare one document to many other documents to identify patterns.

### What is a Document Term Matrix (DTM)?

A DTM or document-feature matrix (DFM) 

Silge and Robinson kept this in mind as they built the `tidytext` package, and included helpful `cast()` functions to turn a tidy text object (again a table with one-token-per-row) into a matrix. 

### Use `cast()` to Convert to a Matrix (Non-tidy) Format

let's create a matrix of all books we looked at

```{r}
#| eval: false

# download corpus
all_books_corp <- gutenberg_download(c(175, # phantom of the opera
                                       42, # jekyll & hyde
                                       84, # frankenstein
                                       345), # dracula
                                     meta_fields = c("title"))
```

```{r}
#| eval: false

# turn corpus into tidy text format
tidy_all_books <- all_books_corp %>% 
    unnest_tokens(output = word, # output col created 
                  input = text # input col that is split
                  ) %>% 
    count(title, word)
```

```{r}
#| eval: false

# convert tidy text table to spare matrix from `Matrix` package
# requires `Matrix` to be installed
all_books_sparse <- tidy_all_books %>% 
    cast_sparse(row = title,
                column = word,
                value = n)
```


```{r}
#| eval: false

# convert tidy text table to DTM object from `tm` package
# requires `tm` to be installed
all_books_dtm <- tidy_all_books %>% 
    cast_dtm(term = word,
             document = title,
             value = n)
```


```{r}
#| eval: false

# convert tidy text table to DFM object from `quanteda` package
# requires `quanteda` to be installed
all_books_dfm <- tidy_all_books %>% 
    cast_dfm(term = word, 
             document = title, 
             value = n)
```



:::{.callout-caution icon=false}
### R Text Mining Tools and Analysis Packages Resources

- [CRAN Task View: Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)
- [Penn Libraries Guides: Text Analysis](https://guides.library.upenn.edu/penntdm/r)
:::

<!--



Many existing text mining tools or algorithms work only on sparse document-term matrices. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices.

TODO:
- add Text vectorization to vocab list: the process of converting text data to numerical vectors. Later those vectors are used to build various machine learning models
- add document-feature matrix (DFM) to vocab list
- add sparse matrix to vocab list
- make sure corpus definition includes metadata info associated with data

NOTES:

Text analytical models (e.g., topic models) often require your data to be stored in a certain format
only so will algorithms be able to quickly compare one document to a lot of other documents to identify patterns
Typically: document-term matrix (DTM), sometimes also called document-feature matrix (DFM)
matrix with each row being a document and each word being a column
term-frequency (tf): The number within each cell describes the number of times the word appears in the document
term frequency–inverse document frequency (tf-idf): weights the occurrence of certain words, e.g., lowering the weight of the word “social” in an corpus of sociological articles

## Exercise: Tidy Text Data from a PDF

- data: DSC plan chapters pdf format
- read in pdf
- turn pdf into a corpus
- remove headers
- tokenize
- rm stop words
    - add custom stop words
- create word cloud


```{r}

```

## Note about python

python has many powerful packages and tools for running analyses like NLP -->
