## Learning Objectives {.unnumbered}

-   Describe principles of tidy text
-   Employ strategies to wrangle unstructured text data into a tidy format using the `tidytext` package
-   Become familiar with text analysis (or text mining) methods and when to use them

::: callout-note
### Acknowledgements

This lesson has been adapted from the following resources:

-   [Welcome to Text Mining with R](https://www.tidytextmining.com/) by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/). Julia and David are also the developers of the [`tidytext`](https://juliasilge.github.io/tidytext/) package.
-   [Section 7.3: (R-) Workflow for Text Analysis from Computational Social Science: Theory & Application, Version: 17 June, 2021](https://bookdown.org/paul/2021_computational_social_science/r-workflow-for-text-analysis.html) by Paul C. Bauer
:::

## What is text data?

Text data is information stored as character or string data types. It comes in various different forms including books, emails, social media posts, interview transcripts, newspapers, government reports, and much more.

### How do we talk about text data?

Here is a list of text data or text analysis terms we'll be referring to throughout this lesson. Note this is not a comprehensive list of text analysis terms that are used beyond this lesson.

| Term                              | Definition                                                                                                                                                                                       |
|---------------|---------------------------------------------------------|
| Corpus (corpora, plural)          | Collection or database of text or multiple texts. These types of objects typically contain raw strings annotated with additional metadata and details.                                           |
| Document-term matrix              | Represents the relationship between terms and documents, where each row stands for a term and each column for a document, and an entry is the number of occurrences of the term in the document. |
| Natural Language Processing (NLP) | NLP is an interdisciplinary field used in computer science, data science, linguistics, and others to analyze, categorize, and work with computerized text.                                       |
| String                            | Specific type of data whose values are enclosed within a set of quotes. Typically values or elements are characters (e.g. "Hello World!").                                                       |
| Text analysis                     | The process of deriving high-quality information or patterns from text through evaluation and interpretation of the output. Also referred to as "text mining" or "text analytics".               |
| Token                             | A meaningful unit of text, such as a word, to use for analysis.                                                                                                                                  |
| Tokenization                      | The process of splitting text into tokens.                                                                                                                                                       |

### How is text data used in the environmental field?

As our knowledge about the environmental world grows, researchers will need new computational approaches for working with text data because reading and identifying all the relevant literature for literature syntheses is becoming an increasingly difficult task.

Beyond literature syntheses, quantitative text analysis tools are extremely valuable for efficiently extracting information from texts and other text mining or text analysis tasks. 

## What is tidy text data?

Let's recall what are the three tidy data principles:

1.  Every column is a variable.
2.  Every row is an observation.
3.  Every cell is a single value.

Keeping that in mind, Silge and Robinson define the tidy text format as **being a table with one-token-per-row**.

This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix.

For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.

By using tidy data principles, we can apply many "tidy" R packages including `dpylr`, `tidyr`, `ggplot2`, and more.

### What is the `tidytext` R package?

::: columns
::: {.column width="30%"}
![](images/tidytext-logo.png){fig-align="center"}
:::

::: {.column width="70%"}
`tidytext` is a package that applies the tidy principles to analyzing text. <br>

The package contains many useful functions to wrangle text data into tidy formats. It has been built considering other text mining R packages so that it's easy to switch between text mining tools (e.g. `quanteda`, `stringr`, `wordcloud2`).
:::
:::

## Exercise: Tidy Text Workflow

![](images/intro-tidy-text-workflow-1.png)

We are going to use the `gutenbergr` package to access public domain texts from [Project Gutenberg](https://www.gutenberg.org/) (a library of free eBooks). We'll then use the `tidytext`, `dyplr` and `ggplot2` packages to practice the tidy text workflow.

Break out into groups and then follow the exercise setup and instructions.

::: {.callout-tip}

### Setup and Instructions

1. Create a new `qmd` file and title it "Intro to Text Data", name yourself as the author, and then save the file as `intro-text-data.qmd`.

2. Create a new code chunk and attach the following libraries:

```{r}
#| message: false
library(gutenbergr) # access public domain texts from Project Gutenberg
library(tidytext) # text mining using tidy tools
library(dplyr) # wrangle data
library(ggplot2) # plot data
```

3. Depending on which group you're in, use one of the following public domain texts:

```{r}
#| eval: false
# Group A
gutenberg_works(title == "Dracula") # dracula text

# Group B
gutenberg_works(title == "Frankenstein; Or, The Modern Prometheus") # frankenstein text

# Group C
gutenberg_works(title == "The Strange Case of Dr. Jekyll and Mr. Hyde") # jekyll hyde text
```

4. Get the id number from the `gutenberg_works()` function so that you can download the text as a corpus using the function `gutenberg_download()`. Save the corpus to an object called `{book-title}_corp`. View the object - is the data in a tidy format?

5. Tokenize the corpus data using `unnest_tokens()`. Take a look at the data - do we need every single token for our analysis?

6. Remove "stop words" or words that can be safely removed or ignored without sacrificing the meaning of the sentence (e.g. "to", "in", "and") using `anti_join()`. Take a look at the data - are you satisfied with your data? We won't conduct any additional cleaning steps here, but consider how you would further clean the data.

7. Calculate the top 10 most frequent words using the functions `count()` and `slice_max()`.

8. Plot the top 10 most frequent words using `ggplot()`. We reccommend creating either a bar plot using `geom_col()` or a lollipop plot using both `geom_point()` and `geom_segment()`.

9. **Bonus**: Consider elements in `theme()` and improve your plot.
:::

### Example using Ray Bradbury's *Asleep in Armageddon*

The code chunks below follows the instructions from above using Ray Bradbury's *Asleep in Armageddon*.

```{r}
# get id number
gutenberg_works(title == "The Phantom of the Opera")
```

```{r}
#| code-fold: true
#| code-summary: Steps 4-7 Code
#| message: false
# access text data using id number from `gutenberg_works()`
phantom_corp <- gutenberg_download(175)

# tidy text data - unnest and remove stop words
tidy_phantom <- phantom_corp %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by = "word")

# calculate top 10 most frequent words
count_phantom <- tidy_phantom %>%
    count(word) %>% 
    slice_max(n = 10, order_by = n)
```

```{r}
#| code-fold: true
#| code-summary: Step 8 Plot Code
# visualize text data #
# bar plot
ggplot(data = count_phantom, aes(n, reorder(word, n))) +
  geom_col() +
    labs(x = "Count",
         y = "Token")
```

```{r}
#| code-fold: true
#| code-summary: Step 9 Plot Code
# visualize text data #
# initial lollipop plot
# ggplot(data = count_phantom, aes(x=word, y=n)) +
#     geom_point() +
#     geom_segment(aes(x=word, xend=word, y=0, yend=n)) +
#     coord_flip() +
#     labs(x = "Token",
#          y = "Count")

# ascending order pretty lollipop plot
ggplot(data = count_phantom, aes(x=reorder(word, n), y=n)) +
    geom_point(color="cyan4") +
    geom_segment(aes(x=word, xend=word, y=0, yend=n), color="cyan4") +
    coord_flip() +
    labs(title = "Top Ten Words in The Phantom of the Opera",
         x = NULL,
         y = "Count") +
    theme_minimal() +
    theme(
        panel.grid.major.y = element_blank()
    )
```


<!--## Tidy Text to Non-Tidy Text Workflow

Many text analysis methods, natural language processing techniques in particular (e.g. topic models) require text data to be stored in a mathematical format. A common approach is to create a document term matrix (DTM), also called a document-feature matrix (DFM). In a matrix format, algorithms will be able to more easily compare one document to many other documents to identify patterns.

## Exercise: DTM Workflow

In the previous exercise, we converted our corpus into a data table that has "one-token-per-row".

Many existing text mining tools or algorithms work only on sparse document-term matrices. Therefore, tidytext provides cast_ verbs for converting from a tidy form to these matrices.

TODO:
- add Text vectorization to vocab list: the process of converting text data to numerical vectors. Later those vectors are used to build various machine learning models
- add document-feature matrix (DFM) to vocab list

NOTES:
What is a Document Term Matrix (DTM)?

Text analytical models (e.g., topic models) often require your data to be stored in a certain format
only so will algorithms be able to quickly compare one document to a lot of other documents to identify patterns
Typically: document-term matrix (DTM), sometimes also called document-feature matrix (DFM)
matrix with each row being a document and each word being a column
term-frequency (tf): The number within each cell describes the number of times the word appears in the document
term frequency–inverse document frequency (tf-idf): weights the occurrence of certain words, e.g., lowering the weight of the word “social” in an corpus of sociological articles

## Exercise: Tidy Text Data from a PDF

- data: DSC plan chapters pdf format
- read in pdf
- turn pdf into a corpus
- remove headers
- tokenize
- rm stop words
    - add custom stop words
- create word cloud

```{r}

```

## Note about python

python has many powerful packages and tools for running analyses like NLP -->
