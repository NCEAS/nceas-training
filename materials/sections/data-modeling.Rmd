## Data Modeling & Tidy Data

### Learning Objectives

-   Understand basics of relational data models aka tidy data
-   Learn how to design and create effective data tables

### Introduction

In this lesson we are going to learn what relational data models are, and how they can be used to manage and analyze data efficiently. Relational data models are what relational databases use to organize tables. However, you don't have to be using a relational database (like mySQL, MariaDB, Oracle, or Microsoft Access) to enjoy the benefits of using a relational data model. Additionally, your data don't have to be large or complex for you to benefit. Here are a few of the benefits of using a relational data model:

-   Powerful search and filtering
-   Handle large, complex data sets
-   Enforce data integrity
-   Decrease errors from redundant updates

#### Simple guidelines for data management {.unnumbered}

A great paper called 'Some Simple Guidelines for Effective Data Management' [@borer_simple_2009] lays out exactly that - guidelines that make your data management, and your reproducible research, more effective. The first six guidelines are straightforward, but worth mentioning here:

- Use a scripted program (like R!)
- Non-proprietary file formats are preferred (eg: csv, txt)
- Keep a raw version of data
- Use descriptive file and variable names (without spaces!)
- Include a header line in your tabular data files
- Use plain ASCII text

The next three are a little more complex, but all are characteristics of the relational data model:

- Design tables to add rows, not columns
- Each column should contain only one type of information
- Record a single piece of data only once; separate information collected at different scales into different tables.

### Recognizing untidy data

Before we learn how to create a relational data model, let's look at how to recognize data that does not conform to the model.

#### Data Organization {-}

This is a screenshot of an actual dataset that came across NCEAS. We have all seen spreadsheets that look like this - and it is fairly obvious that whatever this is, it isn't very tidy. Let's dive deeper in to exactly **why** we wouldn't consider it tidy.

![](images/excel-org-01.png)

#### Multiple tables {-}

Your human brain can see from the way this sheet is laid out that it has three tables within it. Although it is easy for us to see and interpret this, it is extremely difficult to get a computer to see it this way, which will create headaches down the road should you try to read in this information to R or another programming language.

![](images/excel-org-02.png)

#### Inconsistent observations {-}

Rows correspond to **observations**. If you look across a single row, and you notice that there are clearly multiple observations in one row, the data are likely not tidy.

![](images/excel-org-03.png)

#### Inconsistent variables {-}

Columns correspond to **variables**. If you look down a column, and see that multiple variables exist in the table, the data are not tidy. A good test for this can be to see if you think the column consists of only one unit type.

![](images/excel-org-04.png)

#### Marginal sums and statistics {-}

Marginal sums and statistics also are not considered tidy, and they are not the same type of observation as the other rows. Instead, they are a combination of observations.

![](images/excel-org-05.png)

### Good enough data modeling

#### Denormalized data {-}

When data are "denormalized" it means that observations about different entities are combined.

![](images/table-denorm.png) 

In the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individual plants of possibly different species found at that site. This is *not normalized* data.

People often refer to this as *wide* format, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we would have to add new columns to the table. This is difficult to analyze, understand, and maintain.

#### Tabular data {-}

**Observations**. A better way to model data is to organize the observations about each type of entity in its own table. This results in:

-   Separate tables for each type of entity measured

-   Each row represents a single observed entity

-   Observations (rows) are all unique

-   This is *normalized* data (aka *tidy data*)

**Variables**. In addition, for normalized data, we expect the variables to be organized such that:

-   All values in a column are of the same type
-   All columns pertain to the same observed entity (e.g., row)
-   Each column represents either an identifying variable or a measured variable

#### Challenge {- .exercise}

Try to answer the following questions:

What are the observed entities in the example above?

What are the measured variables associated with those observations?

#### {-}


Answer:

![](images/table-denorm-entity-var.png)


If we use these questions to tidy our data, we should end up with:

- one table for each entity observed
- one column for each measured variable
- additional columns for identifying variables (such as site ID)

Here is what our tidy data look like:

![](images/tables-norm.png)

Note that this normalized version of the data meets the three guidelines set by [@borer_simple_2009]:

- Design tables to add rows, not columns
- Each column should contain only one type of information
- Record a single piece of data only once; separate information collected at different scales into different tables.

### Using normalized data

Normalizing data by separating it into multiple tables often makes researchers really uncomfortable. This is understandable! The person who designed this study collected all of these measurements for a reason - so that they could analyze the measurements together. Now that our site and species information are in separate tables, how would we use site elevation as a predictor variable for species composition, for example? The answer is keys - and they are the cornerstone of relational data models.

When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data:

-   Primary Key: unique identifier for each observed entity, one per row
-   Foreign Key: reference to a primary key in another table (linkage)


#### Challenge {- .exercise}

![](images/tables-norm.png)

In our normalized tables above, identify the following:

- the primary key for each table
- any foreign keys that exist

#### {-}

**Answer**

The primary key of the top table is `id`. The primary key of the bottom table is `site`.

The `site` column is the *primary key* of that table because it uniquely identifies each row of the table as a unique observation of a site. In the first table, however, the `site` column is a *foreign key* that references the primary key from the second table. This linkage tells us that the first height measurement for the `DAPU` observation occurred at the site with the name `Taku`.

![](images/tables-keys.png)

#### Entity-Relationship Model (ER) {-}

An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables.

![](images/plotobs-diagram.png)

In the above model, one can see that each site in the sites observations table must have one or more observations in the species observations table, whereas each species opservation has one and only one site.

#### Merging data {-}

Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other.

When conceptualizing merges, one can think of two tables, one on the *left* and one on the *right*. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an *INNER JOIN*. Other types of join are possible as well. A *LEFT JOIN* takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don't match from the left table are still provided with a missing value (na) from the right table. A *RIGHT JOIN* is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a *FULL OUTER JOIN* includes all data from all rows in both tables, and includes missing values wherever necessary.

![](images/join-diagrams.png)

Sometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result.

![](images/sql-joins.png)

In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B.

### Data modeling exercise

-   Break into groups, 1 per table

To demonstrate, we'll be working with a tidied up version of a dataset from ADF&G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: <https://knb.ecoinformatics.org/#view/df35b.304.2>. That site includes metadata describing the full data set, including column definitions. Here's the first `catch` table:

```{r catch, cache=TRUE, echo=FALSE}
library(DT)
catch <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1", method = "libcurl"))
datatable(catch, rownames = FALSE)
```

And here's the `region_defs` table:

```{r regions, cache=TRUE, echo=FALSE}
region_defs <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1", method = "libcurl"))
datatable(region_defs, rownames = FALSE)
```

#### Challenge {- .exercise}

Using the Invision live session your instructor starts, work through the following tasks:

-   Draw an ER model for the tables

    -   Indicate the primary and foreign keys

-   Is the `catch` table in normal (aka tidy) form?

    -   If so, what single type of entity was observed?

    -   If not, how might you restructure the data table to make it tidy?

        -   Draw a new ER diagram showing this re-designed data structure

![](images/ERD_Relationship_Symbols_Quick_Reference-1.png)

#### Optional Extra Challenge {- .exercise}

If you have time, take on this extra challenge with your group.

Navigate to this dataset: [Richard Lanctot and Sarah Saalfeld. 2019. Utqiaġvik shorebird breeding ecology study, Utqiaġvik, Alaska, 2003-2018. Arctic Data Center. doi:10.18739/A23R0PT35](https://doi.org/10.18739/A23R0PT35)

Try to determine the primary and foreign keys for the following tables:

* Utqiagvik_adult_shorebird_banding.csv
* Utqiagvik_egg_measurements.csv
* Utqiagvik_nest_data.csv
* Utqiagvik_shorebird_resightings.csv


### Resources

-   [Borer et al. 2009. **Some Simple Guidelines for Effective Data Management.** Bulletin of the Ecological Society of America.](http://matt.magisa.org/pubs/borer-esa-2009.pdf)
-   [White et al. 2013. **Nine simple ways to make it easier to (re)use your data.** Ideas in Ecology and Evolution 6.](https://doi.org/10.4033/iee.2013.6b.6.f)
-   [Software Carpentry SQL tutorial](https://swcarpentry.github.io/sql-novice-survey/)
-   [Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf)
