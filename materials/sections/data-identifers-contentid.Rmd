## Reproducible Data Access

```{r contentid_load_packages, echo=FALSE}
library(readr)
library(pins)
library(contentid)
```

### Learning Objectives

In this lesson, you will learn:

* Why we strive for reproducible data access
* How content identifiers differ from DOIs
* How content identifiers make research more reproducible
* Ways to register and resolve content identifiers for unpublished data
* How content identifiers can resolve to published data sources

### Barriers to data access

Traditional ways of working with data -- as files on a file system -- limit the 
reproducibility of code to local compute environments. A typical R analysis file will load one or many data files from the local disk with code like this:

```{r contentid-file-load, eval=FALSE}
delta_catch <- readr::read_csv('/Users/jkresearcher/Projects/2018/Delta_Analysis/delta_catch.csv')
delta_taxa <- readr::read_csv('../../Delta_2021/delta_taxa.csv')
delta_effort <- readr::read_csv('delta_effort.csv')
delta_sites <- readr::read_csv('data/delta_sites.csv')
```

Which of those file paths are the most portable? And which will run unmodified on both the original computer that they were written on, and on colleagues' computers? In reality, none of them, in that they require that a specific data file be present in a specific location for the code to work properly, and these assumptions are rarely met and hard to maintain. Hardcoded paths like these are often spread deeply through the scripts that researchers write, and can become a surprise when they are encountered during execution. 

The Web partly solves this problem, because it allows code to access data that is located somewhere on the Internet with a web URI. For example, loading data from a web site can be much more portable than loading the equivalent data from a local computer.

<!-- WARNING MBJ: currently set to NOT eval this block, switch back before final build -->
```{r contentid-load-web, eval=FALSE}
delta_sites_edi <- 'https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&entityid=6a82451e84be1fe82c9821f30ffc2d7d'
delta_sites <- readr::read_csv(delta_sites_edi, show_col_types = FALSE)
head(delta_sites)
```
In theory, that code will work from anyone's computer with an internet connection. But code that downloads data each and every time it is run is not particularly efficient, and will be prohibitive for all but the smallest datasets. A simple solution to this issue is to cache a local copy of the dataset, and only retrieve the original from the web when we don't have a local copy. In this way, people running code or a script will download the data the first time their code is run, but use a local copy from thence forward. While this can be accomplished with some simple conditional logic in R, the pattern has been simplified using the `pins` package:

<!-- WARNING MBJ: currently set to NOT eval this block, switch back before final build -->
```{r contentid-pins, eval=FALSE}
delta_sites_edi <- pins::pin('https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&entityid=6a82451e84be1fe82c9821f30ffc2d7d')
delta_sites <- readr::read_csv(delta_sites_edi, show_col_types = FALSE)
head(delta_sites)
```

You'll note that code takes longer the first time it is run, as the data file is downloaded only the first time. While this works well over the short term, abundant evidence shows that web URIs have short lifespan. Most URIs are defunct within a few years (e.g., see [McCown et al. 2005](http://arxiv.org/abs/cs/0511077)). Only the most carefully curated web sites maintain the viability of their links for longer. And maintaining them for decade-long periods requires a focus on archival principles and dedicated staff to ensure that files and the URLs at which they are published remain accessible. This is precisely the role of archival data repositories like the [Arctic Data Center](https://arcticdata.io), the [KNB Data Repository](https://knb.ecoinformatics.org), and the [Environmental Data Initiative (EDI)](https://portal.edirepository.org).

Finally, no discussion of data access and persistence would be complete without discussing the use of [Digital Object Identifiers (DOIs)](https://datacite.org). DOIs have become the dominant means to create persistent links to academic articles, publications, and datasets. As **authority-based** identifiers, they work when an authority assigns a DOI *name* to a published work, and then ensures that the DOI name always redirects to the current web location of the resource. This is a lot of work, and there is no guarantees that the authorities will keep the links up-to-date. Journals, societies, and data repositories actively maintain the redirection between a DOI such as [doi:10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f](https://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f) and its current location on the EDI Repository. DOIs are commonly assigned to published datasets, and include the bibliographic metadata needed to properly cite and access the dataset.

The challenge with DOIs as they are typically implemented is that they are usually assigned to a [`Dataset`](https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset), which is a collection of digital objects that are composed to form the whole Dataset and that can be accessed individually or through an API. Typically, the metadata attached to DOIs does not include an enumeration of those digital objects or a clear mechanism to get to the actual data -- rather, the DOI redirects to a dataset landing page that provides a human readable summary of the dataset, and often various types of links to find and eventually download the data. Despite advances in metadata interoperability from [DCAT](https://www.w3.org/TR/vocab-dcat-3/#Class:Dataset) and [schema.org/Dataset](https://schema.org/Dataset), there is currently no reliable way to universally go from a known DOI for a dataset to the list of current locations of all of the digital objects that compose that dataset. And yet, this is exactly what we need for portable and persistent data access. In addition, we frequently work with data that doesn't have a DOI yet as we are creating derived data products for analysis locally before they are published. In conclusion, DOIs are a great approach to uniquely citing a dataset, but they do not provde a way for code to download specific, versioned digital objects from a dataset in a portable way that is persistent over many years.

Thus, we want data access to be:

- Portable
- Persistent
- Versioned
- Traceable
- Transparent
- Citable

A powerful approach to solving these problems is by using **content-based** identifiers, rather than authority-based identifiers like DOIs. A content-based identifier, or `contentid` for short, can be calculated from the content in a data file itself, and is unique (within constraints) to that content. This is accomplished by using a "hash" function, which calculates a relatively short, fixed-length, and unique value for any given input. Hash functions form the basis of secure cryptography for secure messaging, and so there are many tools available for conveniently hashing data inputs. In our use case, we can use commonly available cryptographic hash functions (such as `SHA-256` and `SHA-1`) to calculate a unique identifier for any given file. This gives us a unique identifier for the file which can be calculated by anyone with a copy of the file, and which can be registered as metadata in repositories that hold those files.

![](images/contentid-hashes.png)

Once we have a content identifier for an object, we can cache the file locally (just like we did with `pins`), and we can query repositories to see if they contain a copy of that file. Unlike authority-based identifiers, anyone who possesses a copy of a specific version of a data file can calculate the content-identifier for it, enabling us to build systems to find and access those data files across the repository landscape, and really across any web-accessible location. This has all of the power of cacheing and pinning web resources that we demonstrated before, but has the advantage that all holders of the content will use an identical identifier, avoiding broken links. And because content-identifiers can be determined locally before files are published on the web, we can use them in our scripts for data files that have yet to be published and yet know that they will work for others once the files have been published in a repository.

![](images/contentid-registries.png)

### Persistent and portable data access for improving reproducibility

We'll be working with the following IEP dataset that is stored on EDI:

> Interagency Ecological Program (IEP), B. Schreier, B. Davis, and N. Ikemiyagi. 2019. Interagency Ecological Program: Fish catch and water quality data from the Sacramento River floodplain and tidal slough, collected by the Yolo Bypass Fish Monitoring Program, 1998-2018. ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/b0b15aef7f3b52d2c5adc10004c05a6f (Accessed 2021-10-30).

You can [view this IEP dataset on DataONE](https://search.dataone.org/view/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fmetadata%2Feml%2Fedi%2F233%2F2):

![](images/contentid-dataset.png)
It also is visible from the [EDI dataset landing page](https://portal.edirepository.org/nis/mapbrowse?packageid=edi.233.2):

![](images/contentid-edi-landing-page.png)

It contains several data files, each of which is at a specific web URI, including:

- Fish catch and water quality
- Fish taxonomy
- Trap Effort
- Site locations

```{r contentid-dataone-uris, echo=FALSE}
delta_catch_url <- "https://cn.dataone.org/cn/v2/resolve/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F233%2F2%2F015e494911cf35c90089ced5a3127334"
delta_taxa_url <- "https://cn.dataone.org/cn/v2/resolve/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F233%2F2%2F0532048e856d4bd07deea11583b893dd"
delta_effort_url <- "https://cn.dataone.org/cn/v2/resolve/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F233%2F2%2Face1ef25f940866865d24109b7250955"
delta_sites_url <- "https://cn.dataone.org/cn/v2/resolve/https%3A%2F%2Fpasta.lternet.edu%2Fpackage%2Fdata%2Feml%2Fedi%2F233%2F2%2F6a82451e84be1fe82c9821f30ffc2d7d"
```

```{r contentid-edi-uris, echo=TRUE}
delta_catch_edi <- 'https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&entityid=015e494911cf35c90089ced5a3127334'
delta_taxa_edi <- 'https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&entityid=0532048e856d4bd07deea11583b893dd'
delta_effort_edi <- 'https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&entityid=ace1ef25f940866865d24109b7250955'
delta_sites_edi <- 'https://portal.edirepository.org/nis/dataviewer?packageid=edi.233.2&entityid=6a82451e84be1fe82c9821f30ffc2d7d'
```

### Storing a content identifier from a URI

Use the `contentid` package for portable access to data.

<!-- WARNING MBJ: currently set to NOT eval this block, switch back before final build -->
```{r contentid_store, eval=FALSE}
delta_catch_id <- store(delta_catch_url)
delta_taxa_id <- store(delta_taxa_url)
delta_effort_id <- store(delta_effort_url)
delta_sites_id <- store(delta_sites_url)

print(c(delta_catch_id=delta_catch_id, 
        delta_taxa_id=delta_taxa_id,
        delta_effort_id=delta_effort_id, 
        delta_sites_id=delta_sites_id))
```
<!-- WARNING MBJ: currently set to EVAL this block, switch back before final build -->
```{r contentid-remote-build, eval=TRUE, include=FALSE}
delta_catch_id <- 'hash://sha1/317d7f840e598f5f3be732ab0e04f00a8051c6d0'
delta_catch_id <- 'hash://sha256/e0dc10d7f36cfc5ac147956abb91f24f2b2df9f914a004bbf1e85e7d9cf52f41'
delta_taxa_id <- 'hash://sha1/1bf0da8443e5bf8c9e7d16bf715a33129c9ff169'
delta_taxa_id <- 'hash://sha256/1473de800f3c5577da077507fb006be816a9194ddd417b1b98836be92eaea49d'
delta_effort_id <- 'hash://sha256/f2433efab802f55fa28c4aab628f3d529f4fdaf530bbc5c3a67ab92b5e8f71b2'
delta_sites_id <- 'hash://sha256/e25498ffc0208c3ae0e31a23204b856a9309f32ced2c87c8abcdd6f5cef55a9b'
```

### Loading data from a content identifier

```{r contentid-resolve}
delta_taxa_file <- contentid::resolve(delta_taxa_id, store = TRUE)
delta_taxa <- readr::read_csv(delta_taxa_file, show_col_types=FALSE)
 
delta_catch_file <- contentid::resolve(delta_catch_id, store = TRUE)
delta_catch <- readr::read_csv(delta_catch_file, show_col_types=FALSE)
head(delta_catch)
 
delta_sites_file <- contentid::resolve(delta_sites_id, store = TRUE)
delta_sites <- readr::read_csv(delta_sites_file, show_col_types = FALSE)
```

This approach is **portable**, as anyone can run it without having the data local beforehand.
This approach is **persistent**, because it pulls data from persistent archives, and can take advantage of archive redundancy. For example, here is the list of locations that can be currently used to retrieve this data file:

<!-- WARNING MBJ: currently set to only use local repos, switch back before final build -->
```{r list_sources}
contentid::query_sources(delta_catch_id, cols=c("identifier", "source", "date", "status", "sha1", "sha256"), registries = content_dir())
#contentid::query_sources(delta_catch_id, cols=c("identifier", "source", "date", "status", "sha1", "sha256"))

# BUG TO FILE: `query_sources` should not return an error on inaccessible repos -- it should skip them and produce a warning, so that the local repo will still work when disconnected from the internet
```

This approach is **reproducible**, as the exact version will be used every time (even if someone changes the data at the original web URI).

This approach is **traceable** because there is a reference in the code to the specific data used, and the only way to change which data are used is to change the `checksum` thst is being referenced to a new version.

### Publishing local data identifiers

```{r register_local}
# Store a local file
vostok_co2 <- system.file("extdata", "vostok.icecore.co2", package = "contentid")
id <- store(vostok_co2)
vostok <- retrieve(id)
co2 <- read.table(vostok, col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
head(co2)
```

### Improvements to content identifiers

- Content identifiers are not well-linked to DOIs
    - DOIs are the current standard for citing data, and carry the citation metadata for data packages (such as author, title, publication year, etc.)

- Content identifiers are **opaque**, and not particularly transparent
    - Need mechanisms to transparently know what a contentid refers to
    - Luckily, we have everything we need to look up that info in DataONE and similar systems
    
- We can even retrieve the data citation for a content identifier
    
- Final example, showing use of contentid with metadata and a citation

```{r citable_contentid}

```

