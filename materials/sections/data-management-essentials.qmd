## Learning Objectives {.unnumbered}

-   Understand the importance of data management for successfully preserving data
-   Review the Data Life Cycle and how it can guide the data management in a project
-   Familiarize with data management plans and start thinking about one for your project
-   Learn about metadata guidelines and best practices for reproducibility
-   Become familiar with environmental data repositories for accessing and publishing data

## The Big Idea

This lesson aims to get you thinking about how are you going to manage your data. Even though you are in the early stages of defining you research project, we believe that when it comes to data management, the earlier you start thinking about it the better.

<!--*!!!In this lesson we will...!!*-->

## Introduction

Data management is the process of handling, organizing, documenting, and preserving data used in a research project. This is **particularly important in synthesis science** given the nature of synthesis, which involves combining data and information from multiple sources to answer broader questions, generate knowledge and provide insights into a particular problem or phenomenon.

### Benefits of managing your data

Successfully managing your data throughout a research project helps ensures its preservation for future use. It also facilitates collaboration within your team, and it helps advance your scientific outcomes.

**From a researcher perspective**

-   Keep yourself organized -- be able to find your files (data inputs, analytic scripts, outputs at various stages of the analytic process, etc.)
-   Track your science processes for reproducibility -- be able to match up your outputs with exact inputs and transformations that produced them
-   Better control versions of data -- easily identify versions that can be periodically purged
-   Quality control your data more efficiently
-   To avoid data loss (e.g. making backups)
-   Format your data for re-use (by yourself or others)
-   Be prepared to document your data for your own recollection, accountability, and re-use (by yourself or others)
-   Gain credibility and recognition for your science efforts through data sharing!

**Advancement of science**

-   Data is a valuable asset -- it is expensive and time consuming to collect

-   Maximize the effective use and value of data and information assets

-   Continually improve the quality including: data accuracy, integrity, integration, timeliness of data capture and presentation, relevance, and usefulness

-   Ensure appropriate use of data and information

-   Facilitate data sharing

-   Ensure sustainability and accessibility in long term for re-use in science

## The Data Life Cycle

The Data Life Cycle gives you an overview of meaningful steps data goes through in a research project, from planning to archival. This step-by-step breakdown facilitates overseeing individual actions, operations and processes required at each stage. This is a visual tool that aims o help scientists plan and anticipate what will be the *"data needs"* for a specific project (ADD CITATION [Faundeen et al 2013](https://pubs.usgs.gov/of/2013/1265/pdf/of2013-1265.pdf)) .

![Source: Adapted from Faundeen et al 2013, USGS & DataONE](images/data-life-cycle-usgs.png){fig-alt="Data Life Cycle graphic with tools for each step." fig-align="right"}

### Primary Elements

| Step                 | Description                                                                                                                                                                                                                                                                                                                                                                                                                                    | Tool                                                   |
|------------------|------------------------------------|------------------|
| *Plan*               | Map out the processes and resources for all activities related to the handling of the project's data assets. Start with the project goals (desired outputs, outcomes, and impacts) and work backwards to build a data management plan, supporting data policies, and sustainability plans for each step.                                                                                                                                       | Data Management Plan (DPM)                             |
| *Acquire & Discover* | Activities needed to collect new or existing data. You can structure the process of collecting data upfront to better implement data management. Consider data policies and best practices that maintain the provenance and integrity of the data.                                                                                                                                                                                             | Identifying data sources and mechanisms to access data |
| *Process*            | Every step needed to prepare new or existing data to be able to use it as an input for synthesis. Consider the structure of the data, unit transformation, extrapolations, etc                                                                                                                                                                                                                                                                 | Cleaning & Wrangling data skills                       |
| *Integrate*          | Data from multiple sources are combined into a form that can be readily analyzed. Successful data integration depends on documentation of the integration process, clearly citing and making accessible the data you are using, and employing good data management practices throughout the Data Life Cycle. | Modeling & Interpretation                              |
| *Analyze*            | Create analyses and visualizations to identify patterns, test hypotheses, and illustrate findings. During this process, record your methods, document data processing steps, and ensure your data are reproducible. Learn about these best practices and more.                                                                                                                                                                                 | Modeling, Interpretation & Statistics                  |
| *Preserve*           | Plan on how you are going to store your data for long-term use and accessibility so others can access, interpret, and use the data in the future. Decide what data to preserve, where to preserve it, and what documentation needs to accompany the data.                                                                                                                                                                                      | Data packages & repositories                           |
| *Publish and Share*  | Publication and distribution of your data through the web or in data catalogs, social media or other venues to increase the chances of your data being discovered. Data is a research product as publications are.                                                                                                                                                                                                                             | DOIs and citations                                     |

### Cross-Cutting Elements

These elements are involved across all stages describes above. They need to constantly by addressed throughout all the Data Life Cycle, making sure effective data management is in place.

| Step                | Description                                                                                                                                                                                                                                                                                                                                                                                                        | Tool                                             |
|------------------|------------------------------------|------------------|
| *Describe*          | Document data and each of the data stages by describing the why, who, what, when, where, and how of the data and processes. Metadata, or data about data, is key to data sharing and reuse. Documentation such as software code comments, data models, and workflows facilitate indexing, acquiring, understanding, and future uses of the data                                                                    | Metadata and documentation                       |
| *Manage Quality*    | Employ quality assurance and quality control procedures that enhance the quality of data, making sure the measurements or outputs within expected values.Identify potential errors and techniques to address them.                                                                                                                                                                                                 | Quality Control and Quality Assurance techniques |
| *Backup and Secure* | Plan to preserve data in the short term to minimize potential losses (e.g., via software failure, human error, natural disaster). This avoids risk and ensures data is accessible to collaborators. This applies to raw and process data, original science plan, data management plan, data acquisition strategy, processing procedures, versioning, analysis methods, published products, and associated metadata | Servers, secure data sharing services            |

## Data Management Plans

As you can see there is a lot happening around the Data Life Cycle. This is why PLANNING is a key first step. It is advisable to initiate your data management planning at the beginning of your research process before any data has been collected or discovered.

In order to better plan and keep track of all the moving pieces when working with data, a good place to start is creating a Data Management Plan. However, this is not only the starting point. This is a "living" document that should be consulted and updated throughout the project.

A Data Management Plan (DMP) is a document that describes how you will use your data during a research project, as well as what you will do with your data long after the project ends. DMPs should be updated as research plans change to ensure new data management practices are captured ([Environmental Data Initiative](https://edirepository.org/resources/data-management-planning)).

A well-thought-out plan means you are more likely to:

-   stay organized
-   work efficiently
-   truly share data
-   engage your team
-   meet funder requirements as DMPs are becoming common in the submission process for proposals

A DMP is both a straightforward blueprint for how you manage your data, *and* provides guidelines for you and your team on policies, roles and responsibilities. While it is important to plan, it is equally important to recognize that no plan is perfect, as change is inevitable. To make your DMP as robust as possible, review it periodically with your team and adjust as the needs of the project change.

::: callout-note
## How to plan

-   Plan early - information gets lost over time. Think about your data needs as you are starting your project.
-   Plan in collaboration - engaging all the team makes your plan more resilient, including diverse expertise and perspectives.
-   Make revision part of the process - adapt as needed, revising your plan helps you make sure your are on track.
-   Include a tidy data and data ethic lens <!--**!!EXPAND**-->
:::

```{=html}
<!-- **Include tidy and ethical lens:** It is important to start thinking through these lenses during the planning process of your DMP, it will make it easier to include and maintain tidy and ethical principles throughout the entire project. Find information about these topics in the following links to NCEAS Learning Hub materials for a previous course:
        -       [Tidy data](https://learning.nceas.ucsb.edu/2023-06-delta/session_09.html), and our content on 
        -       [FAIR principle](https://learning.nceas.ucsb.edu/2023-06-delta/session_16.html#what-is-fair)
        -       Data ethics lens though the [CARE principles](https://learning.nceas.ucsb.edu/2023-06-delta/session_16.html#what-is-care) later this week. -->
```
### Creating a Good Data management Plan

The article *Ten Simple Rules for Creating a Good Data Management Plan* (@michener2015) outlines the main ideas to keep in mind when you start your "planning stage". Here we summarize each point and provide useful resources to help you achieve this "rules" and write an awesome DMP.

#### Determine what are the organization and/or sponsor requirements

-   Generally, each organization or funding agency have specific expectations on how to manage and disseminate data. Even though a DMP is a good idea to keep you organize. It will save you significant time and effort by first understanding the specific requirements set by the organization you are working for. Each organization often provide templates on how to structure your DMP.

-   Resources

    -   The [DMPTool](https://dmptool.org/) provides templates for different funding agencies plan requirements.
    -   USGS has multiple resources on DMPs. Here the [Data Policy and Guidance](https://www.usgs.gov/programs/climate-adaptation-science-centers/data-policy-and-guidance) for the Climate Adaptation Science Centers

#### Identify the desired/necessary data sets for the project

-   Data is the ultimate reason why we create a DMP. Identifying what data will be use is crusial to planning. Key aspects of the data to consider are:

    -   Type (text, spatial, images, tabualar, etc)

    -   Source (where does the data currently live?, is it propertary data?)

    -   Volume (10 terabytes, 10 megabytes?)

    -   Format (csv, xlsx, shapefiles, etc)

-   Resource

    -   Here is a template spreadsheet to collect all information about the data set you intent to use for your synthesis project. Please make a copy and adapt as needed. <!-- ADD LINK -->

#### Define how the data will be organized

-   Once you know the data you will be using (rule #2) it is time to define how are you going to work with your data. Where will the raw data live? How are the different collaborators going to access the data? The needs vary widely from one project to another depending on the data. When drafting your DMP is helpful to focus on identifying what products and software you will be using. When collaborating with a team it is important to identify f there are any limitations to accessing any software or tool.

-   Resource

    -   [Here is an example](https://nceas.github.io/scicomp.github.io/tutorial_server.html) from the LTER Scientific Computing Support Team on working on NCEAS Server.

#### Explain how the data will be documented

-   We know documenting data is very important. To successfully achieve this we need a plan in place. Three main steps to plan accordingly are:

    -   Identifying the type of infoomation you want/need to collect to document your data throughly

    -   Determine if the is a metadata standard or schema (organized set of elements) you will follow (eg. EML, Dublin Core, ISO 19115, ect). In many cases this relates with what data repository you intend to archive your data.

    -   Establish tools that can help you create and manage metadata content.

-   Resource

    -   [Excel-to EML](https://github.com/lkuiucsb/Excel-to-EML/tree/master) by Li Kui is a workflow that provides a spreadsheet template to collect metadata based on the [Ecological Metadata Language Schema (EML)](https://eml.ecoinformatics.org/)
    -   EDI uses ezEML <!-- ADD LINK AND EXPLANATION -->

#### Describe how data quality will be assured

-   Quality assurance and quality control (QA/QC) are the procedures taken to ensure data looks how we expect it to be. The ultimate goal is to improve the quality of the data products. Some fields of study, data types or funding organizations have specific set of guidelines for QA/QCing data. However, when writing your DMP it is important to describe what measures you plan to take to QA/QC the data (e.g: instrument calibration, verification tests, visualization approaches for error detection, etc.)

-   Resources

    -   Environmental Data Initiative (EDI) description and examples of [Quality Assurance](https://edirepository.org/resources/quality-assurance) and [Quality Control](https://edirepository.org/resources/cleaning-data-and-quality-control)

#### Have a data storage strategy (short and long term)

-   Papers get lost, hardware disk crash, URLs break, different media format degrade. It's inevitable! Plan ahead and think on where your data will live in the short and long-term to ensure the access and use of this data during and long after the project. It is important to have a backup mechanism in place during the project to avoid losing any information.


-   Resource

    -   Remote locations to store your data during your project are: institutionla repositories or servers or commercial services such as Amazone, Dropbox, Google, Microsoft, Box, etc.
    -   Long-term storage: identify an appropriate and stable data repository for your research domain (See section 3.6 [Data Preservation and Sharing](https://learning.nceas.ucsb.edu/2023-08-usgs/session_03.html#data-preservation-sharing))

7.  Define the project's data policies

-   "Many research sponsors require that DMPs include explicit policy statements about how data will be managed and shared. Such policies include:

    -   licensing or sharing arrangements that pertain to the use of preexisting materials;

    -   plans for retaining, licensing, sharing, and embargoing (i.e., limiting use by others for a period of time) data, code, and other materials; and

    -   legal and ethical restrictions on access and use of human subject and other sensitive data". <!-- REVISE AND ADD RESOURCE -->

8.  What data products will be made available and how?

-   "A good dissemination plan includes a few concise statements. State when, how, and what data products will be made available. Generally, making data available to the greatest extent and with the fewest possible restrictions at the time of publication or project completion is encouraged". <!--REVISE AND ADD RESOURCE -->

9.  Assign roles and responsibilities

-   "A comprehensive DMP clearly articulates the roles and responsibilities of every named individual and organization associated with the project. Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration." <!--- Revise and ADD RESOURCE -->

10. Is there a cost associated to managing your data?

-   "Data management takes time and costs money in terms of software, hardware, and personnel. Review your plan and make sure that there are lines in the budget to support the people that manage the data as well as pay for the requisite hardware, software, and services." <!-- REVISE and ADD RESOURCE -->

## Metadata Best Practices

Within the data life cycle you can be collecting data (creating new data) or integrating data that has all ready been collected. Either way, **metadata** plays plays a major role to successfully spin around the cycle because it enables data reuse long after the original collection.

Imagine that you're writing your metadata for a typical researcher (who might even be you!) 30+ years from now - what will they need to understand what's inside your data files?

The goal is to have enough information for the researcher to **understand the data**, **interpret the data**, and then **reuse the data** in another study.

### Overall Guidelines

Another way to think about metadata is to answer the following questions with the documentation:

-   What was measured?
-   Who measured it?
-   When was it measured?
-   Where was it measured?
-   How was it measured?
-   How is the data structured?
-   Why was the data collected?
-   Who should get credit for this data (researcher AND funding agency)?
-   How can this data be reused (licensing)?

### Bibliographic Guidelines

The details that will help your data be cited correctly are:

-   **Global identifier** like a digital object identifier (DOI)
-   Descriptive **title** that includes information about the topic, the geographic location, the dates, and if applicable, the scale of the data
-   Descriptive **abstract** that serves as a brief overview off the specific contents and purpose of the data package
-   **Funding information** like the award number and the sponsor
-   **People and organizations** like the creator of the dataset (i.e. who should be cited), the person to **contact** about the dataset (if different than the creator), and the contributors to the dataset

### Discovery Guidelines

The details that will help your data be discovered correctly are:

-   **Geospatial coverage** of the data, including the field and laboratory sampling locations, place names and precise coordinates
-   **Temporal coverage** of the data, including when the measurements were made and what time period (ie the calendar time or the geologic time) the measurements apply to
-   **Taxonomic coverage** of the data, including what species were measured and what taxonomy standards and procedures were followed
-   Any other **contextual information** as needed

### Interpretation Guidelines

The details that will help your data be interpreted correctly are:

-   **Collection methods** for both field and laboratory data the full experimental and project design as well as how the data in the dataset fits into the overall project
-   **Processing methods** for both field and laboratory samples
-   All sample **quality control procedures**
-   **Provenance** information to support your analysis and modelling methods
-   Information about the **hardware and software** used to process your data, including the make, model, and version
-   **Computing quality control** procedures like testing or code review

### Data Structure and Contents

-   **Everything needs a description**: the data model, the data objects (like tables, images, matrices, spatial layers, etc), and the variables all need to be described so that there is no room for misinterpretation.
-   **Variable information** includes the definition of a variable, a standardized unit of measurement, definitions of any coded values (i.e. 0 = not collected), and any missing values (i.e. 999 = NA).

Not only is this information helpful to you and any other researcher in the future using your data, but it is also helpful to search engines. The semantics of your dataset are crucial to ensure your data is both discoverable by others and interoperable (that is, reusable).

For example, if you were to search for the character string "carbon dioxide flux" in a data repository, not all relevant results will be shown due to varying vocabulary conventions (i.e., "CO2 flux" instead of "carbon dioxide flux") across disciplines --- only datasets containing the exact words "carbon dioxide flux" are returned. With correct semantic annotation of the variables, your dataset that includes information about carbon dioxide flux but that calls it CO2 flux WOULD be included in that search.

### Rights and Attribution

Correctly **assigning a way for your datasets to be cited** and reused is the last piece of a complete metadata document. This section sets the scientific rights and expectations for the future on your data, like:

-   Citation format to be used when giving credit for the data
-   Attribution expectations for the dataset
-   Reuse rights, which describe who may use the data and for what purpose
-   Redistribution rights, which describe who may copy and redistribute the metadata and the data
-   Legal terms and conditions like how the data are licensed for reuse.

### Metadata Standards

So, **how does a computer organize all this information?** There are a number of metadata standards that make your metadata machine readable and therefore easier for data curators to publish your data.

-   [Ecological Metadata Language (EML)](https://eml.ecoinformatics.org/)
-   [Geospatial Metadata Standards (ISO 19115 and ISO 19139)](https://www.fgdc.gov/metadata/iso-standards)
    -   See [NOAA's ISO Workbook](http://www.ncei.noaa.gov/sites/default/files/2020-04/ISO%2019115-2%20Workbook_Part%20II%20Extentions%20for%20imagery%20and%20Gridded%20Data.pdf)
-   [Biological Data Profile (BDP)](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.fgdc.gov/standards/projects/FGDC-standards-projects/metadata/biometadata/biodatap.pdf)
-   [Dublin Core](https://www.dublincore.org/)
-   [Darwin Core](https://dwc.tdwg.org/)
-   [PREservation Metadata: Implementation Strategies (PREMIS)](https://www.loc.gov/standards/premis/)
-   [Metadata Encoding Transmission Standard (METS)](https://www.loc.gov/standards/mets/)

*Note this is not an exhaustive list.*

### Data Identifiers

Many journals require a DOI (a digital object identifier) be assigned to the published data before the paper can be accepted for publication. The reason for that is so that the data can easily be found and easily linked to.

Some data repositories assign a DOI for each dataset you publish on their repository. But, if you need to update the datasets, check the policy of the data repository. Some repositories assign a new DOI after you update the dataset. If this is the case, researchers should cite the exact version of the dataset that they used in their analysis, even if there is a newer version of the dataset available.

### Data Citation

Researchers should get in the habit of citing the data that they use (even if it's their own data!) in each publication that uses that data.

## Data Preservation & Sharing

![](images/WhyManage-small.png)

### Data Packages

> We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve.

Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document.

Many data repositories assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier, often assigned to the metadata and becomes a publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. This allows to identify a digital entity within a data package.

In the graphic to the side, the package can be cited with the DOI `doi:10.5063/F1Z1899CZ`,and each of the individual files have their own identifiers as well.

![](images/data-package.png)

### Data Repositories: Built for Data (and code)

-   GitHub is not an archival location
-   Examples of dedicated data repositories:
    -   KNB
    -   Arctic Data Center
    -   tDAR
    -   EDI
    -   Zenodo
-   Dedicated data repositories are:
    -   Rich in metadata
    -   Archival in their mission
    -   [Certified](https://www.coretrustseal.org/)
-   Data papers, e.g., Scientific Data
-   [re3data](https://www.re3data.org/) is a global registry of research data repositories
-   [Repository Finder](https://repositoryfinder.datacite.org/) is a pilot project and tool to help researchers find an appropriate repository for their work

#### DataOne Federation

DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses.

DataONE can be [searched on the web](https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the (currently 44!) repositories one at a time.

![](images/DataONECNs.png)

## Summary

-   The Data Life Cycle help us see the big picture of our data project.
-   It is extremely helpful to develop a data management plan describing each step of the data life cycle to stay organized.
-   Document everything. Having rich metadata is a key factor to enable data reuse. Describe your data and files and use an appropriate metadata standard.
-   Identify software and tools that will help you and your team organize and document the project's data life cycle.
-   Publish your data in a stable long live repository and assign a unique identifier.

## Activity

## Familiarize with a DMP

-   Look into DMP examples

### How will Data Life Cycle look for your project?

A. Look at the Data Life Cycle diagram and discuss with your neighbor ideas that come to mind on how to plan for each of the stages.

B. Use the logic model you created during the Monday session and answer the following questions:

*Note: Remember this is just a starting point. It is ok to answer to the best of your capacity at the stage where you are with your project*

-   Is there any data going to be created? If yes describe what kind of data and how is it going to be created?
-   Roles and Responsibilities
    -   Who will be the main contact person responsible to keeping the DMP up to date?
    -   Who will be the Data Manages?
    -   Who will be the person responsible for writing and maintaining metadata for the project?
-   What tools and software are you going to use to access, process and check your data?
-   Provide a description of the data that will be used in this project.
    -   Use template <!-- provide link -->
    -   "Description, source, use restrictions, format, fees, quality checks, data processing and scientific workflows, backup and storage, data volume estimates, and data citation. OR Provide a description about how & where this information will be documented throughout the project."
-   Are there any costs associated to data or tools you plan to use?
-   What models do you plan on using for this project? Provide a brief description.
    -   Where will this information de documented?
-   Provide a description/abstract of the data product for your project.
-   Any ideas on how to QA/QC your data? Describe the process for managing the quality of the data throughout the project.
-   Which metadata standard ar you planning on using and why?
-   How will the metadata be created? How will this process be documented though the project?
-   Access and use constraints for your project's data products
    -   Will the re be any requirements or legal restrictions for accessing the data products?
    -   Will the re be any requirements or legal restrictions for usings the data products?
-   Where do you envision archiving the data products for their long-term preservation?
-   How will the data be shared
