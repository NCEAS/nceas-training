@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.name/knitr/},
}

@article{ioannidis_repeatability_2009,
	title = {Repeatability of published microarray gene expression analyses.},
	volume = {41},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/19174838},
	abstract = {Given the complexity of microarray-based gene expression studies, guidelines encourage transparent design and public data availability. Several journals require public data deposition and several public databases exist. However, not all data are publicly available, and even when available, it is unknown whether the published results are reproducible by independent scientists. Here we evaluated the replication of data analyses in 18 articles on microarray-based gene expression profiling published in Nature Genetics in 2005-2006. One table or figure from each article was independently evaluated by two teams of analysts. We reproduced two analyses in principle and six partially or with some discrepancies; ten could not be reproduced. The main reason for failure to reproduce was data unavailability, and discrepancies were mostly due to incomplete data annotation or specification of data processing and analysis. Repeatability of published microarray studies is apparently limited. More strict publication rules enforcing public data availability and explicit description of data processing and analysis should be considered.},
	number = {2},
	journal = {Nature genetics},
	author = {Ioannidis, John P A and Allison, David B and Ball, Catherine A and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aedín C and Falchi, Mario and Furlanello, Cesare and Game, Laurence and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and Nitzberg, Michael and Page, Grier P and Petretto, Enrico and Van Noort, Vera},
	year = {2009},
	pages = {149--155}
}

@article{ioannidis_why_2005,
	title = {Why most published research findings are false.},
	volume = {2},
	issn = {15491676},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16060722},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	number = {8},
	journal = {PLoS Medicine},
	author = {Ioannidis, John P A},
	year = {2005},
	keywords = {bias (epidemiology), Data Interpretation, likelihood functions, meta analysis topic, odds ratio, Publishing, reproducibility results, Research Design, sample size, Statistical},
	pages = {e124},
	file = {Attachment:/Users/jones/Library/Application Support/Firefox/Profiles/n7afwwxd.default/zotero/storage/U23G3S3W/Ioannidis - 2005 - Why most published research findings are false.pdf:application/pdf}
}

@article{ioannidis_why_2012,
	title = {Why {Science} {Is} {Not} {Necessarily} {Self}-{Correcting}},
	volume = {7},
	issn = {1745-6916, 1745-6924},
	url = {http://pps.sagepub.com/content/7/6/645},
	doi = {10.1177/1745691612464056},
	abstract = {The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that “it is obvious that progress is made” is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.},
	language = {en},
	number = {6},
	urldate = {2015-10-22},
	journal = {Perspectives on Psychological Science},
	author = {Ioannidis, John P. A.},
	month = nov,
	year = {2012},
	pmid = {26168125},
	keywords = {replication, self-correction},
	pages = {645--654},
	file = {Full Text PDF:/Users/jones/Library/Application Support/Firefox/Profiles/n7afwwxd.default/zotero/storage/CRWPZJ6H/Ioannidis - 2012 - Why Science Is Not Necessarily Self-Correcting.pdf:application/pdf;Snapshot:/Users/jones/Library/Application Support/Firefox/Profiles/n7afwwxd.default/zotero/storage/NUKTTJDU/645.full.html:text/html}
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	number = {1},
	urldate = {2017-11-18},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	pages = {0021},
	file = {munafo-repro-manifesto-2017-s41562-016-0021.pdf:/Users/jones/Library/Application Support/Firefox/Profiles/n7afwwxd.default/zotero/storage/345WHUNN/munafo-repro-manifesto-2017-s41562-016-0021.pdf:application/pdf}
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	language = {en},
	number = {6251},
	urldate = {2017-11-26},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pages = {aac4716--aac4716}
}

@article{hampton_tao_2015,
	title = {The {Tao} of {Open} {Science} for {Ecology}},
	volume = {6},
	doi = {http://dx.doi.org/10.1890/ES14-00402.1},
	journal = {Ecosphere},
	author = {Hampton, Stephanie E and Anderson, Sean and Bagby, Sarah C and Gries, Corinna and Han, Xueying and Hart, Edmund and Jones, Matthew B and Lenhardt, W Christopher and MacDonald, Andrew and Michener, William and Mudge, JF and A, Pourmokhtarian and Schildhauer, Mark and Woo, KH and Zimmerman, N},
	month = jul,
	year = {2015}
}

@article{marwick_packaging_2017,
 title = {Packaging data analytical work reproducibly using R (and friends)},
 author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
 year = 2017,
 month = aug,
 keywords = {reproducible research, data science, social computing, computer education, statistics, Scientific Computing and Simulation, Programming Languages},
 abstract = {
        Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.
      },
 volume = 5,
 pages = {e3192v1},
 journal = {PeerJ Preprints},
 issn = {2167-9843},
 url = {https://doi.org/10.7287/peerj.preprints.3192v1},
 doi = {10.7287/peerj.preprints.3192v1}
}

@Manual{marwick_rrtools_010,
    title = {rrtools: Creates a reproducible research compendium},
    year = 2017,
    author = {Ben Marwick},
    note = {R package version 0.1.0},
    url = {https://github.com/benmarwick/rrtools},
}

@techreport{fraser_questionable_2018,
	title = {Questionable {Research} {Practices} in {Ecology} and {Evolution}},
	url = {https://osf.io/ajyqg/},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	urldate = {2018-03-29},
	institution = {Open Science Framework},
	author = {Fraser, Hannah and Parker, Timothy and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	year = {2018},
	doi = {10.17605/OSF.IO/AJYQG},
	pages = {--}
}
