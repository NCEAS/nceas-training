---
title: "Text Mining"
author: "Julien Brun"
output: html_document
---


# Goal

The Goal of this session is to learn how to mine PDFs to get information out of them. Text mining encompasses a vast field of theoretical approaches and methods with one thing in common: text as input information (Feiner et al, 2008) 


# Which R packages are available?

Looking at the Natural Language Processing (NLP) CRAN view, you will realize there are a lot of different packages to accomplish this complex task : <https://cran.r-project.org/web/views/NaturalLanguageProcessing.html>.

Here are some important packages:

- `tm`: provides a framework and the algorithmic background for mining text
- `quanteda`: A fast and flexible framework for the management, processing, and quantitative analysis of textual data in R. It has very nice features, among which include finding specific words and their context in the text
- `tidytext`: provides means for text mining for word processing and sentiment analysis using dplyr, ggplot2, and other tidy tools

***=> In this quick introduction we are going to use `quanteda`***


# Analyzing peer-reviewed journal articles about BP Deep Horizon's oil spill

First, let us load the necessary packages

```{r,warning=F,message=F}
library("readtext")
library("quanteda")
```

## 1. Import the PDFs into R

```{r}
# set path to the PDF (here on Aurora)
pdf_path <- "/tmp/oil_spill_pdfs"

# List the PDFs about the BP oil spill
pdfs <- list.files(path = pdf_path, pattern = 'pdf$',  full.names = TRUE) 

# Import the PDFs into R
spill_texts <- readtext(pdfs, 
                        docvarsfrom = "filenames", 
                        sep = "_", 
                        docvarnames = c("First_author", "Year"))
```

## 2. Create the Corpus object needed for the text analysis

```{r}
# Transform the journal articles into a corpus object
spill_corpus  <- corpus(spill_texts)

# Some stats about the journal articles
tokenInfo <- summary(spill_corpus)
```

### Add metadata to the Corpus object

For example we can add the information that these texts are written in English.
```{r}
# add metadata to files, in this case that they are written in english
metadoc(spill_corpus, 'language') <- "english" 

# visualize corpus structure and contents, now with added metadata
summary(spill_corpus, showmeta = T)
```

### Subset coprus

Do you want only articles before 2017?

```{r}
summary(corpus_subset(spill_corpus, Year < 2017))
```

### Search for words with context: 4 words on each side of the keyword

```{r, results="hide"}
kwic(spill_corpus, "dispersant", 4)
```


## 3. Build a Document-Feature Matrix (DFM) 

More information about DFM can be found on Quanteda's vignette: http://quanteda.io/articles/quickstart.html. In a nutshell, additional rules can be applied on top of the tokenization process, such as ignoring certain words, punctuation, case, ...

```{r}
# construct the DFM, which is the base object to further analyze the journal articles
spills_DFM <- dfm(spill_corpus, tolower = TRUE, stem = FALSE, 
                  remove = c("et", "al", "fig", "table", "ml", "http",
                             stopwords("SMART")),
                  remove_punct = TRUE, remove_numbers = TRUE)

# returns the top 20 frequent words
topfeatures(spills_DFM, 20) 
```

Note: You can check what words are listed by default in stopwords:
```{r}
head(stopwords("english"), 20)
```

## 4. Extract information from a Document-Feature Matrix (DFM) 

### Word cloud

Quickly visualize the most frequent words:

```{r}
# set the seed for wordcloud
set.seed(1)

# plots wordcloud
textplot_wordcloud(spills_DFM, min.freq = 60, random.order=F, 
                   rot.per = .10,  
                   colors = RColorBrewer::brewer.pal(8,'Dark2')) 
```


### Grouping documents by metadata

Here we are grouping the documents by year of publication:

```{r}
spills_DFM_yearly <- dfm(spill_corpus, groups = "Year", tolower = TRUE, stem = TRUE, 
                  remove = c("et", "al", "fig", "table", "ml", "http",
                             stopwords("SMART")),
                  remove_punct = TRUE, remove_numbers = TRUE)

# Sort by year and show the top 20 most frequent words
dfm_sort(spills_DFM_yearly)[,1:20]
```

### Searching for concepts using sets of keywords

One very powerful feature of `quanteda` is to allow to group keywords by dictionary to mine texts. 

```{r}
myDict <- dictionary(list(pollution = c("oil", "oiled", "crude", "petroleum", "pahs", "pah", "tph", "benzo", "hydrocarbons", "pollution"),
                          measurement = c("data", "sample", "samples", "sampling", "study")))
```

```{r}
spills_DFM <- dfm(spill_corpus, dictionary = myDict)

spills_DFM
```

***The above text manipulations are the necessary steps to enable more advanced text analaysis, such as topical modeling and similarities between texts.***


# References and sources

- Book on text mining in R: http://tidytextmining.com/
- `tm` package: https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf
- `quanteda` package:
    - Overview: https://github.com/kbenoit/quanteda
    - Getting started: http://quanteda.io/articles/quickstart.html
- Munzert, Simon. Automated Data Collection with R: A Practical Guide to Web Scraping and Text Mining. Chichester, West Sussex, United Kingdom: John Wiley & Sons Inc, 2015.
- [Text Mining with R](http://tidytextmining.com): A Tidy Approach, by Julia Silge and David Robinson.
